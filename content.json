{"pages":[],"posts":[{"title":"Application of Selenium: Auto Daily Health Report","text":"March.1.2020-March.4.2020 SeleniumSometimes we might forget upload our health status to the daily health report website, and the absent record will keep for 14 days, which is annoying. By a lucky coincidence, I noticed that selenium can implement browser operations, so I decided to use it to solve the problem. I only need to use this program to log in the website and click the submit button. 1234567891011121314151617181920212223242526272829303132333435#coding=utf-8from selenium import webdriverfrom selenium.webdriver.chrome.options import Optionsfrom time import sleepchrome_options = Options()chrome_options.add_argument('--headless') # 16年之后，chrome给出的解决办法，抢了PhantomJS饭碗chrome_options.add_argument('--disable-gpu')chrome_options.add_argument('--no-sandbox') # root用户不加这条会无法运行driver = webdriver.Chrome(chrome_options=chrome_options)driver.implicitly_wait(10)driver.get(&quot;https://passport.ustc.edu.cn/login?service=https%3A%2F%2Fweixine.ustc.edu.cn%2F2020%2Fcaslogin&quot;)print('当前页面为 %s' %driver.title)elementUser = driver.find_element_by_id(&quot;username&quot;)elementPasswort = driver.find_element_by_id(&quot;password&quot;)elementUser.send_keys(' ')elementPasswort.send_keys(' \\n') #隐私起见删掉了自己的账号密码print('当前页面为 %s' %driver.title)# etest = driver.find_element_by_name('other_detail')# etest.send_keys('test')sleep(5)driver.find_element_by_id('report-submit-btn').click()sleep(5)driver.close() CrontabThen I uploaded the program to my server. By using crontab to run this program regularly. 10 0-23/10 * * * /usr/bin/python3 /root/daka.py This means I will clock in to at 00:00, 10:00 and 20:00 everyday. May.22.2021 Updates今天学校微信突然通知说我今天没打卡,发现竟然把我这个阿里云服务器的ip封了,只要用这个ip打卡就会需要输入验证码,但是验证码图片根本加载不出来(有理由相信就是没有相应的验证码图片).说白了就是不能用这个ip打卡了,估计是之前打卡的时间过于规律.于是换ip,改用vultr的另外一个服务器,加上了random随机数,随机等待一段时间再打卡而不是一打开网页就打卡,问题解决. May.26.2021 Updates今天又不行了,我一看,验证码竟然实装了,但是这也问题不大,我看了一眼验证码都比较简单.利用百度的OCR(Optical Character Recognition)接口,进行图像识别,逐次尝试直到打卡成功,问题解决. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125#coding=utf-8from selenium import webdriverfrom selenium.webdriver.chrome.options import Optionsfrom time import sleepimport randomfrom aip import AipOcrimport refindTime = re.compile(r'：(.+?)，')&quot;&quot;&quot; 读取图片 &quot;&quot;&quot;def get_file_content(filePath): with open(filePath, 'rb') as fp: return fp.read()&quot;&quot;&quot; 你的APPID AK SK &quot;&quot;&quot;APP_ID = ' 'API_KEY = ' 'SECRET_KEY = ' ' #这里各位就填自己的吧,隐私起见我把我的删了client = AipOcr(APP_ID, API_KEY, SECRET_KEY)chrome_options = Options()chrome_options.add_argument('--headless') # 16年之后，chrome给出的解决办法，抢了PhantomJS饭碗chrome_options.add_argument('--disable-gpu')chrome_options.add_argument('--no-sandbox') # root用户不加这条会无法运行driver = webdriver.Chrome(chrome_options=chrome_options)driver.implicitly_wait(10)sleep(random.randint(0, 1000))loop = 1while loop != -1: driver.get(&quot;https://passport.ustc.edu.cn/login?service=https%3A%2F%2Fweixine.ustc.edu.cn%2F2020%2Fcaslogin&quot;) print('当前页面为 %s' % driver.title) try: elementUser = driver.find_element_by_id(&quot;username&quot;) elementPasswort = driver.find_element_by_id(&quot;password&quot;) elementUser.send_keys('') elementPasswort.send_keys('\\n') #隐私起见删掉了自己的账号密码 except: pass print('当前页面为 %s' % driver.title) # driver.get('https://weixine.ustc.edu.cn/2020/home') try: sleep(3) pic = driver.find_element_by_id('captcha-box') pic.screenshot('yzm.png') image = get_file_content('yzm.png') options = {} options[&quot;probability&quot;] = &quot;true&quot; &quot;&quot;&quot; 调用通用文字识别, 图片参数为本地图片 &quot;&quot;&quot; Result = client.basicGeneral(image, options) #Result=client.basicGeneralUrl(url,options) #print(Result[&quot;words_result_num&quot;]) show = Result['words_result'] for i in show: print(i['words']) yzm = i['words'] print(show) yzm = yzm.replace(' ', '') yzm = yzm.replace('.', '') yzm = yzm.replace('-', '') print(yzm) elementyzm = driver.find_element_by_xpath('//input[@class=&quot;form-control&quot;][@name=&quot;captcha&quot;]') elementyzm.send_keys(yzm) except: print('无需验证码') pass try: driver.find_element_by_xpath( '//input[@type=&quot;radio&quot;and@name=&quot;now_address&quot;and@value=&quot;1&quot;]/following-sibling::i').click() sleep(1) driver.find_element_by_xpath( '//input[@type=&quot;radio&quot;and@name=&quot;is_inschool&quot;and@value=&quot;2&quot;]/following-sibling::i').click() sleep(1) driver.find_element_by_xpath('//input[@type=&quot;radio&quot;and@name=&quot;has_fever&quot;and@value=&quot;0&quot;]/following-sibling::i').click() sleep(1) driver.find_element_by_xpath( '//input[@type=&quot;radio&quot;and@name=&quot;last_touch_sars&quot;and@value=&quot;0&quot;]/following-sibling::i').click() options = driver.find_elements_by_xpath('//button[@class=&quot;btn dropdown-toggle btn-default&quot;]') options[0].click() driver.find_element_by_xpath('//span[contains(text(),&quot;安徽省&quot;)]/..').click() sleep(1) options[1].click() driver.find_element_by_xpath('//span[contains(text(),&quot;合肥市&quot;)]/..').click() sleep(1) options[2].click() driver.find_element_by_xpath('//span[contains(text(),&quot;正常&quot;)]/..').click() sleep(1) options[3].click() driver.find_element_by_xpath('//span[contains(text(),&quot;正常在校园内&quot;)]/..').click() sleep(1) except: pass driver.find_element_by_id('report-submit-btn').click() warn_text = driver.find_element_by_xpath('//strong[not(contains(@class,&quot;text-danger&quot;))]') result = warn_text.text print(result) loop = result.find('验证码') print('本次loop = %d' % loop) sleep(5) # driver.close()driver.quit()","link":"/2021/03/23/Application-of-Selenium-Auto-Daily-Health-Report/"},{"title":"CNN with Pytorch","text":"CNN with PytorchBasic","link":"/2023/04/11/CNN-with-Pytorch/"},{"title":"Diffusion model","text":"A Diffusion Model from Scratch in Pytorch Investigating the dataset Building the Diffusion Model Step 1: The forward process = Noise scheduler Step 2: The backward process = U-Net Step 3: The loss Sampling Training Ways to Improve A Diffusion Model from Scratch in PytorchReposted from https://www.youtube.com/watch?v=a4Yfz2FxXiY&amp;t=884s In this notebook I want to build a very simple (as few code as possible) Diffusion Model for generating car images. I will explain all the theoretical details in the YouTube video. Sources: Github implementation Denoising Diffusion Pytorch Niels Rogge, Kashif Rasul, Huggingface notebook Papers on Diffusion models ([Dhariwal, Nichol, 2021], [Ho et al., 2020] ect.) Investigating the datasetAs dataset we use the StandordCars Dataset, which consists of around 8000 images in the train set. Let’s see if this is enough to get good results ;-) 12345678910111213141516import torchimport torchvisionimport matplotlib.pyplot as pltimport mathdef show_images(dataset, num_samples=20, cols=4): &quot;&quot;&quot; Plots some samples from the dataset &quot;&quot;&quot; plt.figure(figsize=(15,15)) for i, img in enumerate(dataset): if i == num_samples: break plt.subplot(math.ceil(num_samples/cols + 1), cols, i + 1) plt.imshow(img[0])data = torchvision.datasets.StanfordCars(root=&quot;.&quot;, download=True)show_images(data) Later in this notebook we will do some additional modifications to this dataset, for example make the images smaller, convert them to tensors ect. Building the Diffusion ModelStep 1: The forward process = Noise schedulerWe first need to build the inputs for our model, which are more and more noisy images. Instead of doing this sequentially, we can use the closed form provided in the papers to calculate the image for any of the timesteps individually. Key Takeaways: The noise-levels/variances can be pre-computed There are different types of variance schedules We can sample each timestep image independently (Sums of Gaussians is also Gaussian) No model is needed in this forward step 1234567891011121314151617181920212223242526272829303132333435363738394041import torch.nn.functional as Fdef linear_beta_schedule(timesteps, start=0.0001, end=0.02): return torch.linspace(start, end, timesteps)def get_index_from_list(vals, t, x_shape): &quot;&quot;&quot; Returns a specific index t of a passed list of values vals while considering the batch dimension. &quot;&quot;&quot; batch_size = t.shape[0] out = vals.gather(-1, t.cpu()) return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)def forward_diffusion_sample(x_0, t, device=&quot;cpu&quot;): &quot;&quot;&quot; Takes an image and a timestep as input and returns the noisy version of it &quot;&quot;&quot; noise = torch.randn_like(x_0) sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape) sqrt_one_minus_alphas_cumprod_t = get_index_from_list( sqrt_one_minus_alphas_cumprod, t, x_0.shape ) # mean + variance return sqrt_alphas_cumprod_t.to(device) * x_0.to(device) \\ + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)# Define beta scheduleT = 300betas = linear_beta_schedule(timesteps=T)# Pre-calculate different terms for closed formalphas = 1. - betasalphas_cumprod = torch.cumprod(alphas, axis=0)alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)sqrt_recip_alphas = torch.sqrt(1.0 / alphas)sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod) Let’s test it on our dataset … 1234567891011121314151617181920212223242526272829303132333435363738from torchvision import transforms from torch.utils.data import DataLoaderimport numpy as npIMG_SIZE = 64BATCH_SIZE = 128def load_transformed_dataset(): data_transforms = [ transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.RandomHorizontalFlip(), transforms.ToTensor(), # Scales data into [0,1] transforms.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1] ] data_transform = transforms.Compose(data_transforms) train = torchvision.datasets.StanfordCars(root=&quot;.&quot;, download=True, transform=data_transform) test = torchvision.datasets.StanfordCars(root=&quot;.&quot;, download=True, transform=data_transform, split='test') return torch.utils.data.ConcatDataset([train, test])def show_tensor_image(image): reverse_transforms = transforms.Compose([ transforms.Lambda(lambda t: (t + 1) / 2), transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC transforms.Lambda(lambda t: t * 255.), transforms.Lambda(lambda t: t.numpy().astype(np.uint8)), transforms.ToPILImage(), ]) # Take first image of batch if len(image.shape) == 4: image = image[0, :, :, :] plt.imshow(reverse_transforms(image))data = load_transformed_dataset()dataloader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True) 12345678910111213# Simulate forward diffusionimage = next(iter(dataloader))[0]plt.figure(figsize=(15,15))plt.axis('off')num_images = 10stepsize = int(T/num_images)for idx in range(0, T, stepsize): t = torch.Tensor([idx]).type(torch.int64) plt.subplot(1, num_images+1, math.ceil(idx/stepsize) + 1) image, noise = forward_diffusion_sample(image, t) show_tensor_image(image) Step 2: The backward process = U-NetFor a great introduction to UNets, have a look at this post: https://amaarora.github.io/2020/09/13/unet.html. Key Takeaways: We use a simple form of a UNet for to predict the noise in the image The input is a noisy image, the ouput the noise in the image Because the parameters are shared accross time, we need to tell the network in which timestep we are The Timestep is encoded by the transformer Sinusoidal Embedding We output one single value (mean), because the variance is fixed 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103from torch import nnimport mathclass Block(nn.Module): def __init__(self, in_ch, out_ch, time_emb_dim, up=False): super().__init__() self.time_mlp = nn.Linear(time_emb_dim, out_ch) if up: self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1) self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1) else: self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1) self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1) self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1) self.bnorm1 = nn.BatchNorm2d(out_ch) self.bnorm2 = nn.BatchNorm2d(out_ch) self.relu = nn.ReLU() def forward(self, x, t, ): # First Conv h = self.bnorm1(self.relu(self.conv1(x))) # Time embedding time_emb = self.relu(self.time_mlp(t)) # Extend last 2 dimensions time_emb = time_emb[(..., ) + (None, ) * 2] # Add time channel h = h + time_emb # Second Conv h = self.bnorm2(self.relu(self.conv2(h))) # Down or Upsample return self.transform(h)class SinusoidalPositionEmbeddings(nn.Module): def __init__(self, dim): super().__init__() self.dim = dim def forward(self, time): device = time.device half_dim = self.dim // 2 embeddings = math.log(10000) / (half_dim - 1) embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings) embeddings = time[:, None] * embeddings[None, :] embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1) # TODO: Double check the ordering here return embeddingsclass SimpleUnet(nn.Module): &quot;&quot;&quot; A simplified variant of the Unet architecture. &quot;&quot;&quot; def __init__(self): super().__init__() image_channels = 3 down_channels = (64, 128, 256, 512, 1024) up_channels = (1024, 512, 256, 128, 64) out_dim = 1 time_emb_dim = 32 # Time embedding self.time_mlp = nn.Sequential( SinusoidalPositionEmbeddings(time_emb_dim), nn.Linear(time_emb_dim, time_emb_dim), nn.ReLU() ) # Initial projection self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1) # Downsample self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], \\ time_emb_dim) \\ for i in range(len(down_channels)-1)]) # Upsample self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], \\ time_emb_dim, up=True) \\ for i in range(len(up_channels)-1)]) self.output = nn.Conv2d(up_channels[-1], 3, out_dim) def forward(self, x, timestep): # Embedd time t = self.time_mlp(timestep) # Initial conv x = self.conv0(x) # Unet residual_inputs = [] for down in self.downs: x = down(x, t) residual_inputs.append(x) for up in self.ups: residual_x = residual_inputs.pop() # Add residual x as additional channels x = torch.cat((x, residual_x), dim=1) x = up(x, t) return self.output(x)model = SimpleUnet()print(&quot;Num params: &quot;, sum(p.numel() for p in model.parameters()))model Further improvements that can be implemented: Residual connections Different activation functions like SiLU, GWLU, … BatchNormalization GroupNormalization Attention … Step 3: The lossKey Takeaways: After some maths we end up with a very simple loss function There are other possible choices like L2 loss ect. 1234def get_loss(model, x_0, t): x_noisy, noise = forward_diffusion_sample(x_0, t, device) noise_pred = model(x_noisy, t) return F.l1_loss(noise, noise_pred) Sampling Without adding @torch.no_grad() we quickly run out of memory, because pytorch tacks all the previous images for gradient calculation Because we pre-calculated the noise variances for the forward pass, we also have to use them when we sequentially perform the backward process 123456789101112131415161718192021222324252627282930313233343536373839404142@torch.no_grad()def sample_timestep(x, t): &quot;&quot;&quot; Calls the model to predict the noise in the image and returns the denoised image. Applies noise to this image, if we are not in the last step yet. &quot;&quot;&quot; betas_t = get_index_from_list(betas, t, x.shape) sqrt_one_minus_alphas_cumprod_t = get_index_from_list( sqrt_one_minus_alphas_cumprod, t, x.shape ) sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape) # Call model (current image - noise prediction) model_mean = sqrt_recip_alphas_t * ( x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t ) posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape) if t == 0: return model_mean else: noise = torch.randn_like(x) return model_mean + torch.sqrt(posterior_variance_t) * noise @torch.no_grad()def sample_plot_image(): # Sample noise img_size = IMG_SIZE img = torch.randn((1, 3, img_size, img_size), device=device) plt.figure(figsize=(15,15)) plt.axis('off') num_images = 10 stepsize = int(T/num_images) for i in range(0,T)[::-1]: t = torch.full((1,), i, device=device, dtype=torch.long) img = sample_timestep(img, t) if i % stepsize == 0: plt.subplot(1, num_images, int(i/stepsize)+1) show_tensor_image(img.detach().cpu()) plt.show() Training1print(torch.cuda.is_available()) 1234567891011121314151617181920from torch.optim import Adamdevice = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;model.to(device)optimizer = Adam(model.parameters(), lr=0.001)epochs = 100 # Try more!for epoch in range(epochs): for step, batch in enumerate(dataloader): optimizer.zero_grad() t = torch.randint(0, T, (BATCH_SIZE,), device=device).long() loss = get_loss(model, batch[0], t) loss.backward() optimizer.step() if epoch % 5 == 0 and step == 0: print(f&quot;Epoch {epoch} | step {step:03d} Loss: {loss.item()} &quot;) sample_plot_image() In Table 2, we show the sample quality effects of reverse process parameterizations and trainingobjectives (Section 3.2). We find that the baseline option of predicting µ˜ works well only whentrained on the true variational bound instead of unweighted mean squared error, a simplified objectiveakin to Eq. (14). We also see that learning reverse process variances (by incorporating a parameterizeddiagonal Σθ(xt) into the variational bound) leads to unstable training and poorer sample qualitycompared to fixed variances. Predicting , as we proposed, performs approximately as well aspredicting µ˜ when trained on the variational bound with fixed variances, but much better when trainedwith our simplified objective. iffusion models scale down the data with each forward process step (by a √1 − βt factor)so that variance does not grow when adding noise, thus providing consistently scaled inputsto the neural net reverse process. NCSN omits this scaling factor. Ways to Improve loss function value of $\\beta_t$","link":"/2023/04/03/Diffusion-model/"},{"title":"Enhancing Anime","text":"Why We Choose This Topic References Why We Choose This Topic事情的起因是看到了Intel ISL实验室做的这么一个东西[1]. 第一想法是,既然能够实现生成更加真实的图片,那么这种方法应该也能实现二次元风格的转换. 目前的一些基于deep learning, convolutional networks, adversarial training的技术,绕过了传统渲染流程的几何建模后设置材料与光照的流程,而是通过使用大量数据训练的CNN,来生成指定图片[2][3][4].生成的图片很多情况下可以以假乱真.但另一方面,这些合成是高度不可控的,与驱动游戏画面的渲染管道完全分离.ISL的想法应该是将CG与ML结合,利用CG的可控性与ML的高质量,从而将游戏画面提升到下一世代. ISL创新的地方在于,摄取了G-buffers中的坐标,材料,照明的数据作为图像增强网络的输入提供,提供了一个足够稳定并且连续的输入. 鉴于之前已经有Crypko团队做的高质量立绘,感觉实现二次元画风转换也不会太遥远. 上面这个Demo,如果我输入的参数足够连续,是不是能生成一个足够连续的结果?如果能应用在游戏上会怎么样?这个是我一开始的想法. 其实ISL做的Enhancing photorealism enhancement具体实现我还没仔细看_(:3」∠)_ 大致看了一下他应该是对不同类型的物体采取了不同的策略,转二次元画风最重要的是人物,或者说是人物的脸 (好像身材也有点重要,看过那些为了最后渲染出来合适的身材,把实际建模身体比例拉的很夸张的例子),至少环境差别并不大,或者说完全用传统NPR就够了,重点针对人物处理就可以了. 想象一下,以后能有这么一个游戏,每一帧的画面都和插画一样精致,人物就像是无缝衔接的丝滑立绘,那不是吊打现在所有三渲二的赛璐珞画风. 总而言之,个人觉得优势点在于: $\\bullet$ 现在游戏行业用的三渲二都是基于传统的NPR,日式卡通渲染技术点主要是描边、边缘光、头发高光、面部修正[5][6][7]之类的,为了实现”二次元”的感觉,必须用各种不符合物理常识的”奇淫巧计”来实现,比如“隐藏额发投影”,“强行让光照方向与场景无关,只与角色朝向和视角有关”之类的这些人为处理,需要手工调整,非常的费时,而且再怎么提升也几乎不可能达到手绘的效果,相比之下采用CNN强化渲染的潜力就很大了. $\\bullet$ 虽然ML发展非常迅速,但是少有人将其运用在游戏行业的画面提升方面,这方面的竞争相对也更小. $\\bullet$ 二次元图片挺好找的,网上那么多图库,而且我也爬了几百G图片了,画风比较统一的画师也不少. 参考偷懒放上了知乎的一些回答 :) References[1] Stephan R. Richter, Hassan Abu AlHaija, and Vladlen Koltun. Enhancing photorealism enhancement. $arXiv:2105.04619$, 2021[2] T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing of GANs for improved quality, stability, and variation,” in $International Conference on Learning Representations$, 2018.[3] A. Brock, J. Donahue, and K. Simonyan, “Large scale GAN training for high fidelity natural image synthesis,” in $International Conference on Learning Representations$, 2019.[4] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture for generative adversarial networks,” in $Computer Vision and Pattern Recognition$, 2019.[5] flashyiyi. 到目前为止的二次元渲染总结 [6] Hugh86. Unity NPR之日式卡通渲染（基础篇） [7] 幕后煮屎人. 基于PBR的NPR着色器","link":"/2021/05/24/Enhancing-Anime/"},{"title":"Lattice Boltzmann Method","text":"Lattice Boltzmann Method圆形障碍物速度场 涡度 密度 矩形障碍物速度场 涡度 密度","link":"/2022/05/28/Lattice-Boltzmann-Method/"},{"title":"Notes of Machine Learning(P12-P17)","text":"Deep Learning Deep is Better? Modularization Gradient Descent - Intro of Backpropagation Chain Rule Backpropagation Tips for Deep Learning Regularization Dropout CNN(Convolutional Neural Networks) Why CNN for Image The whole CNN CNN-Convolution CNN-Max Pooling Flatten CNN in Keras What does CNN learn? Deep LearningDeep is Better?通常相同参数数量的情况下, Fat + Short 不如Thin + Tall 的效果好. 因为很tall,所以形象的有个deep learning的说法(DNN-Deep Neural Networks). Modularization$\\cdot$ Deep $\\rightarrow$ Modularization 相当于把整个的分类分成若干个小的分类: 每层提取相应的attribute. 开始两节课似乎都是简要介绍.接下来才有干货. Gradient Descent - Intro of Backpropagation我们的神经网络有若干个待定的参数$\\theta = \\{w_1,w_2,…,b_1,b_2,…\\}$,随机选取初始值$\\theta_0$,回忆之前学过的内容,最简单的Gradient Descent是说,我们首先要计算Loss Function关于各个参数的偏导: \\nabla L(\\theta) = \\left[ \\begin{array}{l} \\partial L(\\theta)/\\partial w_1 \\\\ \\partial L(\\theta)/\\partial w_2 \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots \\\\ \\partial L(\\theta)/\\partial b_1 \\\\ \\partial L(\\theta)/\\partial b_2 \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots \\end{array} \\right]然后得到下一步的参数: \\theta^1 = \\theta^0 - \\eta \\nabla L(\\theta^0)以此类推,理想情况下最终可以收敛得到想要的结果.但是在实际应用场景中,比如语音辨识系统,神经网络通常会有七八层,每层有1k个neuron,于是会需要上百万个参数,计算起来会非常复杂,Backpropagation就是有效计算这个超长向量的偏导的方法. Chain RuleCase 1. $y = g(x)$, $z = h(y)$ \\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx}Case 2. $x = g(s)$, $y = h(s)$, $z = k(x,y)$ \\frac{dz}{ds} = \\frac{\\partial z}{\\partial x} \\frac{dx}{ds} + \\frac{\\partial z}{\\partial y} \\frac{dy}{ds}很基础的数学. L(\\theta) = \\sum^N_{n = 1} C^n(\\theta) \\rightarrow \\frac{\\partial L(\\theta)}{\\partial w} = \\sum_{n=1}^N \\frac{\\partial C^n(\\theta)}{\\partial w}Backpropagation取其中一个节点分析: 我们最终需要计算的是$\\frac{\\partial C}{\\partial w}$,可以通过链式法则等效于计算$\\frac{\\partial z}{\\partial w} \\frac{\\partial C}{\\partial z}$. 其中,计算$\\frac{\\partial z}{\\partial w}$被称作Forward pass,计算$\\frac{\\partial C}{\\partial z}$被称作Backward pass. $\\frac{\\partial z}{\\partial w}$就是前一个节点的output,不需要额外计算. 得到关系: \\frac{\\partial C}{\\partial z} = \\sigma'(z)[w_3 \\frac{\\partial C}{\\partial z'}+w_4 \\frac{\\partial C}{\\partial z''}]这是往前推一层的计算,如果有很多层,就逐层套公式即可. 最终可以得到每个需要计算的偏导. Tips for Deep Learning对于太深的网络,如果每个节点都用softmax会导致前面的层数”学习”的非常慢,可能后面的几层早就训练好了,但是前面的还没怎么动.解决办法是使用Rectified Linear Unit(ReLU). ReLU也有几种变种Leaky ReLU, Parametric ReLU. 还有人提出了Maxout, ReLU可以看作Maxout的特殊情况(ReLU is a special cases of Maxout) Regularization修改一下Loss Function. L'(\\theta) = L(\\theta) + \\lambda \\frac{1}{2}||\\theta||_1DropoutEach time before updating the parameters: $\\cdot$ Each neuro has p% to dropout $\\cdot$ Using the new network for training For testing: If the dropout rate is p%, all the weights times (100-p)% CNN(Convolutional Neural Networks)Why CNN for Image如果使用fully connected的neural networks往往需要太多的参数.所以可以人为滤掉一些用不上的weights. $\\cdot$ Property 1. Some patterns are much smaller than the whole image.(A neuron does not have to see the whole image to discover the pattern, every neuron only need to connect to small region with less parameters) $\\cdot$ Property 2. The same patterns appear in different regions. $\\cdot$ Property 3. Subsampling the pixels will not change the object. The whole CNN CNN-Convolution输入一个图像矩阵,我们有相应的若干个Filter,而这些Filter矩阵中的元素就是这个网络的参数. 移动Filter依次做内积，得到结果也可以用一个矩阵表示. 得到的值最大的地方就是特征出现的地方.做的这个处理实际上相当于是去掉了一些weight的neural network.比起fully connected的neural network来说参数更少. P.S. 对于彩色图片,相当于输入一个三维矩阵,高度为3(RGB三种颜色),Filter也是三维矩阵. CNN-Max Pooling缩小image的方法,在每个选定大小的区块中,只取他的最大值. Convolution $\\rightarrow$ Max Pooling 的过程可以重复很多次.每次循环都会使图像的size变小. Flatten把feature map拉直.接着再丢进Fully Connected Feedforward network. CNN in Keras基础的Keras指令见这篇博客:https://sakura-gh.github.io/ML-notes/ML-notes-html/10_Keras.html 想添加CNN网络,在Keras中的指令很简单. 1model.add(Convolution2D(25, 3, 3, input_shape=(1, 28, 28))) 这一步是Convolution.添加25个$3 \\times 3$的Filter. 1model.add(MaxPooling2D((2, 2))) 这一步是Max Pooling,每次取$2 \\times 2$的矩阵中的最大值. 做Convolution之前,input的dimension是$1 \\times 28 \\times 28$,做一次Convolution之后维度是$25 \\times 25 \\times 26$.因为有25个Filter.做Max Pooling之后维度是$25 \\times 13 \\times 13$,因为$2 \\times 2$的范围选择相当于每个维度大小除以2.对于每个Filter的参数数量,第一次Convolution的时候是$3 \\times 3 = 9$,第二次Convolution的Filter是50个$3 \\times 3$的矩阵,此时每个Filter的参数数量为$25 \\times 9 = 225$(深度为25). 在实操的时候发现这些个函数的语法都变了,Convolution2D改成了Conv2D之类的.具体怎么操作还是及时去看官方文档更好一点. What does CNN learn? 定义. Degree of the activation of the k-th filter: $a^k = \\sum^{11}_{i = 1} \\sum^{11}_{j = 1}a^k_{ij}$ 用gradient ascent寻找: x^* = arg \\ \\mathop{\\rm{max}}_x a^k当输入为$x$时,特定的filter其中的某一层会最活跃.这里例举了某个filter的前12层例子,可以给我们提供可视化的信息.(寻找让卷积核最大激活的一个$x$) 对于Flatten之后的神经网络,如果我们使得 x^* = arg \\ \\mathop{\\rm{max}}_x y^i 我们是不是能看到对应的数字?答案是并不会. 想找个办法使得这个图看起来像数字.考虑: x^* = arg \\ \\mathop{\\rm{max}}_x (y^i - \\sum_{i,j}|x_{ij}|)添加了一项惩罚项,也就是说我们希望大部分的地方是没有涂颜色的. (其实感觉还是一般) 老师最后给了几个更好的,利用机器学习生成图片的方法: $\\cdot$ PixelRNN https://arxiv.org/abs/1601.06759 $\\cdot$ Variation Autoencoder (VAE) https://arxiv.org/abs/1312.6114 $\\cdot$ Generative Adversarial Network (GAN) https://arxiv.org/abs/1406.2661","link":"/2021/06/04/Notes-of-Machine-Learning-P12/"},{"title":"Notes of Machine Learning(P18-P19)","text":"Graph Neural Networks Graph GNN: How? GNN Roadmap Spatial-based Convolution NN4G (Neural Networks for Graph) DCNN (Diffusion-Convolution Neural Network) DGC (Diffusion Graph Convolution) MoNET (Mixture Model Networks) GraphSAGE GAT (Graph Attention Networks) GIN (Graph Isomorphism Network) Graph Neural NetworksGraphNode &amp; edge (节点和边) 在很多应用场景, Unlabeled Node的数量是远大于Labled Node的数量的. GNN: How?$\\cdot$ how to embed node into a feature space using convolution? 一个Graph怎么做卷积呢? Solution 1: Generalize the concept of convolution (corelation) to graph $\\rightarrow$ Spatial-based convolution Solution 2: Back to the defination of convolution in signal processing $\\rightarrow$ Spectral-based convolution 分别是基于空间与基于频谱的卷积. GNN Roadmap 接着具体讲一下这两种Solution Spatial-based ConvolutionTerminology: $\\cdot$ Aggregate: 用neighbor feature update下一层的hidden state $\\cdot$ Readout: 把所有nodes的feature集合起来代表整个graph NN4G (Neural Networks for Graph)先做Aggregate: Readout: 每一层都求和取平均,再经过一个节点输出 DCNN (Diffusion-Convolution Neural Network)Aggregate: 每一层都是在最初始的graph上计算,只是每一层取的距离不一样,第一层计算与每个node相距为1的node的平均值,第二计算与每个node相距为2的node的平均值(自己也算与自己相距为2的node),以此类推. Readout: DGC (Diffusion Graph Convolution)与上一个方法的Readout不一样, DGC直接把结果全部加起来 MoNET (Mixture Model Networks)$\\cdot$ Define a measure on node ‘distance’. $\\cdot$ Use weighted sum (mean) instead of simply summing up (averaging) neighbor features. GraphSAGE GAT (Graph Attention Networks) GIN (Graph Isomorphism Network)$\\cdot$ A GNN can be at most as powerful as WL isomorphic test $\\cdot$ Theoretical proofs were provided","link":"/2021/06/24/Notes-of-Machine-Learning-P18/"},{"title":"Notes of Machine Learning(P20-P21)","text":"Recurrent Neural Network(RNN) Example Application Learning Recurrent Neural Network(RNN)Example Application$\\cdot$ Slot Filling 自动识别输入的一串文字中哪些是需要的有效信息. e.g. input:’我明天要去上海. ‘ 从中提取’明天’和’上海’,但简单的DNN分类会遇到问题,比如无法区分目的地和出发地,这些判别需要用到上下文来判断,这是单纯的DNN做不到的. 于是考虑RNN: 额外输出一组结果,作为下一次的输入,这样相当于考虑了之前的输入的影响. Of course it can be deep… $\\cdot$ Bidirectional RNN 还有同时双向考虑的Bidirectional RNN,可以同时考虑上下文 $\\cdot$ LSTM (Long Short-term Memory) Learning$\\cdot$ Backpropagation through time(BPTT)","link":"/2021/07/22/Notes-of-Machine-Learning-P20/"},{"title":"Notes of Machine Learning(P22-P23)","text":"Semi-supervised Learning Outline Semi-supervised Learning for Generative Model Supervised Generative Model Semi-supervised Generative Model Why? Semi-supervised Learning Low-density Separation Self-training Outlook: Semi-supervised SVM Smoothness Assumption Graph-based Approach Graph-based Approach - Graph Construction Unsupervised Learning 1-of-N Encoding Word Embedding How to exploit the context? Semi-supervised Learning之前了解的都是Supervised Learning: $\\{(x^r,\\hat{y}^r)\\}^R_{r=1}$ 数据集$R$的每个$x^r$都有对应的标签$\\hat{y}^r$ Semi-supervised learning: $\\{(x^r,\\hat{y}^r)\\}^R_{r=1},\\{x^u\\}^{R+U}_{u=R}$​ ​ $\\cdot$ A set of unlabeled data, usually U &gt;&gt; R ​ $\\cdot$ Transductive learning: unlabeled data is the testing data ​ $\\cdot$ Inductive learning: unlabeled data is not the testing data Why semi-supervised learning? ​ $\\cdot$ Collecting data is easy, but collecting “labelled” data is expensive(深有同感) ​ $\\cdot$ We do semi-supervised learning in our lives Outline$\\cdot$ Semi-supervised Learning for Generative Model $\\cdot$ Low-density Separation Assumption $\\cdot$ Smoothness Assumption $\\cdot$ Better Representation Semi-supervised Learning for Generative ModelSupervised Generative Model每笔数据都有label,可以直接算出$\\mu^i, \\Sigma$ 如上图所示 Semi-supervised Generative ModelUnlabeled data也会对$\\mu^i, \\Sigma$产生影响 步骤如下 $\\cdot$​ Initialization: $\\theta = \\{P(C_1),P(C_2),\\mu^1,\\mu^2,\\Sigma\\}$​ $\\cdot$​ Step 1: compute the posterior probability of unlabeled data $P_{\\theta}(C_1|x^u)$​ $P$ depending on model $\\theta$ $\\cdot$ Step 2: update model P(C_1) = \\frac{N_1+\\sum_{x^u}P(C_1|x^u)}{N}其中 $N$: total number of examples $N_1$: number of examples belonging to $C_1$ 对于$\\mu$​, labeled data直接计算, unlabeled data用概率计算期望值 \\mu^1 = \\frac{1}{N_1}\\sum_{x^r \\in C_1} x^r + \\frac{1}{\\sum_{x^u}P(C_1|x^u)}\\sum_{x^u}P(C_1|x^u)x^uthen back to step 1. Why?$\\bullet$​ Maximum likelihood with labelled data log L(\\theta) = \\sum_{x^r}logP_\\theta (x^r,\\hat{y}^r)其中 P_\\theta(x^r,\\hat{y}^r) = P_\\theta(x^r|\\hat{y}^r)P(\\hat{y}^r)$P_\\theta(x^r|\\hat{y}^r)$是$\\hat{y}^r$这个class产生$x^r$的几率​ $\\bullet$​ Maximum likelihood with labelled + unlabelede data logL(\\theta) = \\sum_{x^r}logP_\\theta (x^r,\\hat{y}^r) + \\sum_{x^u}logP_\\theta (x^u)其中 P_\\theta (x^u) = P_\\theta (x^u|C_1)P(C_1) + P_\\theta (x^u|C_2)P(C_2)Semi-supervised Learning Low-density SeparationSelf-training$\\bullet$ Given: labelled data set = $\\{(x^r,\\hat{y}^r)\\}^R_{r=1}$, unlabeled data set = $\\{x^u\\}^{R+U}_{u=R}$​​ $\\bullet$ Repeat: ​ $\\cdot$ Train model $f^*$ from labelled data set (which is Independent to the model $\\theta$) ​ $\\cdot$ Apply $f^*$ to the unlabeled data set ​ $\\cdot$ Obtain $\\{(x^u,y^u)\\}^{R+U}_{u=l}$​ (Pseudo-label) ​ $\\cdot$ Remove a set of data from unlabeled data set, and add them into labeled data set $\\bullet$ Similar to semi-supervised learning for generative model $\\bullet$ Hard label v.s. Soft label ​ Considering using neural network ​ $\\theta^*$ (network parameter) from labelled data soft效果不好,一般都用hard 上述内容有进阶版 $\\downarrow$ $\\bullet$ Entropy-based Regularization Output的distribution要很集中 Entropy of $y^u$: Evaluate how concentrate the distribution $y^u$​ is E(y^u) = - \\sum^5_{m=1}y^u_mln(y^u_m)$E$要尽可能小 (公式这里的5是因为举的例子里有$y$有5个class) 然后可以重新定义Loss Function L = \\sum_{x^r}C(y^r,\\hat{y}^r) + \\lambda \\sum_{x^u}E(y^u)右边第一项对应labelled data,第二项对应unlabeled data 除此之外,semi-supervised learning还有别的方法 Outlook: Semi-supervised SVM穷举所有可能的情况 取margin尽可能大的情况 (第二种) Smoothness Assumption$\\bullet$​ Assumption: “similar” $x$ has the same $\\hat{y}$​ $\\bullet$ More precisely: ​ $\\cdot$ $x$​ is not uniform ($x$​的分布是不平均的,在某些地方很集中,某些地方又很分散) ​ $\\cdot$ If $x^1$ and $x^2$ are closed in a high density region, $\\hat{y}^1$ and $\\hat{y}^2$ are the same $x_1$和$x_2$通过一个high density的path相连 应用: Classify astronomy v.s. travel articles Graph-based Approach$\\bullet$​ How to know $x_1$ and $x_2$ are closed in a high density region (connected by a high density path) Represented the data points as a graph But how to build a graph? $\\cdot$ Graph representation is nature sometimes. E.g. Hyperlink of webpages, citation of papers $\\cdot$ Sometimes you have to construct the graph yourself. Graph-based Approach - Graph Construction$\\bullet$ Define the similarities $s(x^i,x^j)$ between $x^i$ and $x^j$​ $\\bullet$ Add edge: ​ $\\cdot$ K Nearest Neighbor ​ $\\cdot$ $e$​-Neighborhood $\\bullet$ Edge weight is proportional to $s(x^i,x^j)$ 比较推荐的选择是Gaussian Radial Basis Function: s(x^i,x^j) = exp(-\\gamma ||x^i-x^j||^2)$\\bullet$ Define the smoothness of the labels on the graph 显然左边的看起来更加smooth,不过最好是需要一个定量描述 S = \\frac{1}{2}\\sum_{i,j}w_{i,j}(y^i-y^j)^2根据定义可以算得 这个$S$值越小表示这个graph越光滑 这个表达式还可以进一步简化 S = \\frac{1}{2}\\sum_{i,j}w_{i,j}(y^i-y^j)^2 = \\textbf{y}^TL\\textbf{y}其中$\\textbf y$: (R+U)-dim vector \\textbf{y} = [...y^i...y^j]^T$L$: (R+U)$\\times$(R+U) matrix, 被称为Graph Laplacian L = D -W W = \\begin{bmatrix} 0 & 2 & 3 & 0\\\\ 2 & 0 & 2 & 0\\\\ 3 & 1 & 0 & 1\\\\ 0 & 0 & 1 & 0 \\end{bmatrix}$D$是把$W$的每一行加起来放在对角线位置所构成的矩阵 D = \\begin{bmatrix} 5 & 0 & 0 & 0\\\\ 0 & 3 & 0 & 0\\\\ 0 & 0 & 5 & 0\\\\ 0 & 0 & 0 & 1 \\end{bmatrix}可以得到Loss Function(此$L$与之前的矩阵$L$不一样) L = \\sum_{x^r}C(y^r,\\hat{y}^r) + \\lambda S然后按照Gradient Descent的方法Train就行了 同时,这个smooth的要求并不一定是要在output layer,中间的hidden layer也可以有smooth的要求,最终就在Loss Function中加上这些项即可 Unsupervised Learning1-of-N Encoding每个word都用一个vector来表示,对应向量空间中的其中一个维度,但是会有如下缺点: 一是vector需要的维度数量是所有可能出现的word的数量,一般来说会非常大. 二是不能提供更多的分类信息,比如狗是[1 0 0 0 0],猫是[0 1 0 0 0],但是他们都属于动物,不能简单地从对应的向量中看出对应的关系. Word Embedding 怎么做word embedding? $\\cdot$ Machine learn ther meaning of words from reading a lot of documents without supervision. 最终期望的效果是输入一个word,经过神经网络后,输出的是一个word embedding的向量,训练素材就是一大堆的文本素材.训练过程是无监督的. 学习的关键是单词的含义可以通过上下文来得到. How to exploit the context? Count based if two words $w_i$ and $w_j$ frequently co-occur, $V(w_i)$ and $V(w_j)$ would be close to each other Prediction based","link":"/2021/08/07/Notes-of-Machine-Learning-P22/"},{"title":"TensorFlow Tutorial","text":"TensorFlow 2.x Tutorial Pandas Load data into pandas Reading data in Pandas Sorting/Describing Data Making changes to the data Saving Data (Exporting into Desired Format) Filtering Data Conditional Changes Aggregate Statistics (Groupby) Working with large amounts of data Numpy Load in numPy The basic Get dimension Get shape Get type Get size Get total size Get a specific element [row, column] Get a specific row Get a specific column [start_index:end_index:stepsize] Change value 3-d example Initializing different types of arrays Operation Program Element in TensorFlow Constant Variable Cast Elementwise Slicing, indexing Reshaping numpy Differences between variable and constant Keras Load data Model Loss and optimizer Training Evaluate Predictions Another way to do predictions: model + softmax Slicing Linear Regression Load data Clean data Convert categorical ‘Origin’ data into one-hot data Split the data into train and test Split features from labels Plot Normalize Regression TensorFlow 2.x TutorialPandasLoad data into pandas123456789101112import pandas as pddf = pd.read_csv('pokemon_data.csv')# print(df.head(5))# df_xlsx = pd.read_excel('pokemon_data.xlsx')# print(df_xlsx.head(3))# df = pd.read_csv('pokemon_data.txt', sep='\\t')# print(df.head(5))df['HP'] Reading data in Pandas1234567891011121314#### Read Headersdf.columns## Read each Columnprint(df[['Name', 'Type 1', 'HP']])## Read Each Rowprint(df.iloc[0:4])for index, row in df.iterrows(): print(index, row['Name'])df.loc[df['Type 1'] == &quot;Grass&quot;]## Read a specific location (R,C)print(df.iloc[2,1]) Sorting/Describing Data123df.describe()df.sort_values(['Type 1', 'HP'], ascending=[1,0]) # 1 is for ascending, 0 is for descending, or just use True or False Making changes to the data123456789# df['Total'] = df['HP'] + df['Attack'] + df['Defense'] + df['Sp. Atk'] + df['Sp. Def'] + df['Speed']# df = df.drop(columns=['Total'])df['Total'] = df.iloc[:, 4:10].sum(axis=1) #do the same thing as the first rowcols = list(df.columns)df = df[cols[0:4] + [cols[-1]] + cols[4:12]]df.head(5) Saving Data (Exporting into Desired Format)12345# df.to_csv('modified.csv', index=False)# df.to_excel('modified.xlsx', index=False)df.to_csv('modified.txt', index=False, sep='\\t') Filtering Data123456789101112import renew_df = df.loc[(df['Type 1'] == 'Grass') &amp; (df['Type 2'] == 'Poison') &amp; (df['HP'] &gt; 70)]new_df = new_df.loc[~df['Name'].str.contains('Mega')] # delete all the pokemons that have 'Mega' in their names# df.loc[df['Type 1'].str.contains('Fire|Grass', regex=True)]# df.loc[df['Type 1'].str.contains('^pi[a-z]*', flags=re.I, regex=True)]new_df.reset_index(drop=True, inplace=True) # drop is to drop the previous index, inplace is to change the value of itselfnew_df.to_csv('filtered.csv') Conditional Changes123df.loc[df['Type 1'] == 'Fire', 'Type 1'] = 'Flamer'# df.loc[df['Total'] &gt; 500, ['Generation','Legendary']] = ['Test 1', 'Test 2'] Aggregate Statistics (Groupby)123456789df = pd.read_csv('modified.csv')df.groupby(['Type 1']).mean()df.groupby(['Type 1']).mean().sort_values('Defense', ascending=False)df.groupby(['Type 1']).sum()# df.groupby(['Type 1']).count()df['count'] = 1df.groupby(['Type 1', 'Type 2']).count()['count'] Working with large amounts of data123for df in pd.read_csv('modified.csv', chunksize=5): results = df.groupby(['Type 1']).count() new_df = pd.concat([new_df, results]) NumpyLoad in numPy1import numpy as np The basic12a = np.array([1,2,3], dtype='int32')print(a) Get dimension12a.ndim# 1 Get shape12a.shape# (3,) Get type12a.dtype# dtype('int32') Get size12a.itemsize# 4 Get total size1234a.size * a.itemsize# 12a.nbytes# 12 Get a specific element [row, column]123a = np.array([[1,2,3,4,5,6,7],[8,9,10,11,12,13,14]])a[1, 5]# 13 Get a specific row12a[0, :]# array([1, 2, 3, 4, 5, 6, 7]) Get a specific column12a[:, 2]# array([ 3, 10]) [start_index:end_index:stepsize]12a[0, 1:6:2]# array([2, 4, 6]) Change value123456789101112131415161718a[1, 5] = 20print(a)'''[[ 1 2 3 4 5 6 7] [ 8 9 10 11 12 20 14]]'''a[:, 2] = 5print(a)'''[[ 1 2 5 4 5 6 7] [ 8 9 5 11 12 20 14]]'''a[:, 2] = [1, 2]print(a)'''[[ 1 2 1 4 5 6 7] [ 8 9 2 11 12 20 14]]''' 3-d example12345678910111213141516171819b = np.array([[[1,2],[3,4]],[[5,6],[7,8]]])b[0,1,1]# 4b[:,1,:]'''array([[3, 4], [7, 8]])'''# replaceb[:,1,:] = [[8,8],[9,9]]b'''array([[[1, 2], [8, 8]], [[5, 6], [9, 9]]])''' Initializing different types of arrays123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778# All 0s matrixnp.zeros((3,5))# ornp.zeros([3,5])# All 1s matrixnp.ones((2,3))# Any other numbernp.full((2,2), 99)# Any other number (full_like)np.full_like(a, 4)'''array([[4, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 4, 4, 4]])'''# Random decimal numbersnp.random.rand(2,2,3,4)'''array([[[[0.74976159, 0.24543374, 0.67686477, 0.73581296], [0.493751 , 0.80378955, 0.89915921, 0.73957771], [0.28789667, 0.01167291, 0.24646214, 0.24632085]], [[0.51345246, 0.71863008, 0.21579854, 0.74275805], [0.86887821, 0.75579047, 0.8425766 , 0.79416401], [0.2859915 , 0.28921223, 0.91217814, 0.871138 ]]], [[[0.34372382, 0.58492713, 0.66887337, 0.23508036], [0.35333139, 0.47615369, 0.54830846, 0.85649478], [0.53456592, 0.89527531, 0.62903241, 0.96490809]], [[0.05080588, 0.67885852, 0.43191936, 0.86446833], [0.78385384, 0.08768826, 0.00214857, 0.88836965], [0.70274751, 0.308583 , 0.53971867, 0.74181569]]]])'''# Random int numbersnp.random.randint(-4,8,size=(3,3))'''array([[ 5, -1, 3], [-1, 3, 4], [-4, 7, 3]])'''# Identity matrixnp.identity(5)'''array([[1., 0., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 0., 1., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]])'''# Repeat an arrayarr = np.array([1,2,3])r1 = np.repeat(arr,3)print(r1)# [1 1 1 2 2 2 3 3 3]arr = np.array([[1,2,3]])r1 = np.repeat(arr,3,axis=0)print(r1)'''[[1 2 3] [1 2 3] [1 2 3]]'''np.random.random_sample(a.shape)'''array([[0.92590309, 0.69170996, 0.86595192, 0.32043676, 0.78114367, 0.73246055, 0.13357556], [0.9876799 , 0.17386839, 0.34905811, 0.58150665, 0.74796994, 0.62469343, 0.10405232]])''' Operation1234a = np.array([1, 3, 5])b = np.array([1, 2, 3])a * b# np.array([1, 6, 15]) Program Element in TensorFlowConstantExample: 123a = tf.constant(2.0, tf.float32)b = tf.constant(3.0)c = tf.constant([4, 7, 9, 2], shape=(2,2), dtype=tf.float32) VariableExample: 1234567W = tf.Variable([.3], dtype=tf.float32)b = tf.Variable([-.3], dtype=tf.float32)x = tf.ones((3, 3))y = tf.eye(3)z = tf.random.normal((2, 3), mean=0, stddev=1)z = tf.random.uniform((3, 3), minval=0, maxval=1)z = tf.range(10) Down below are some operations you can do to variables Cast1x = tf.cast(x, dtype=tf.float32) Elementwise12345678910111213x = tf.constant([1, 2, 3])y = tf.constant([4, 5, 6])z = x + yz = x - yz = x / yz = x * yz = x ** 3z = tf.tensordot(x, y, axes=1) # 点乘x = tf.random.normal((2, 3))y = tf.random.normal((3, 2))z = tf.matmul(x, y) # 矩阵相乘z = x @ y # it is the same thing as above Slicing, indexing12345678910x = tf.constant([[1, 2, 3, 4], [5, 6, 7, 8]])print(x[0])# tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)print(x[:, 0])# tf.Tensor([1 5], shape=(2,), dtype=int32)print(x[0, :])# tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)print(x[0, 1:3])# tf.Tensor([2 3], shape=(2,), dtype=int32)print(x[0, 1]) Reshaping12345678910111213141516x = tf.random.normal((2, 3))print(x)&quot;&quot;&quot;tf.Tensor([[ 0.15445794 -0.9360153 1.8880193 ] [ 1.705707 0.6765113 -0.5381024 ]], shape=(2, 3), dtype=float32)&quot;&quot;&quot;x = tf.reshape(x, (3, 2))print(x)&quot;&quot;&quot;tf.Tensor([[ 0.15445794 -0.9360153 ] [ 1.8880193 1.705707 ] [ 0.6765113 -0.5381024 ]], shape=(3, 2), dtype=float32)&quot;&quot;&quot;x = tf.reshape(x, (-1, 2)) # it is the same thing as above numpy12345678910111213import numpy as npx = tf.random.normal((2, 3))print(type(x))# &lt;class 'tensorflow.python.framework.ops.EagerTensor'&gt;x = x.numpy()print(type(x))# &lt;class 'numpy.ndarray'&gt;x = tf.convert_to_tensor(x)print(type(x))# &lt;class 'tensorflow.python.framework.ops.EagerTensor'&gt;x = tf.constant([&quot;Patrick&quot;, &quot;Max&quot;, &quot;Mary&quot;])print(type(x))# &lt;class 'tensorflow.python.framework.ops.EagerTensor'&gt; Differences between variable and constant1234567x = tf.constant([1, 2, 3])print(x)# tf.Tensor([1 2 3], shape=(3,), dtype=int32)x = tf.Variable([1, 2, 3])print(x)# &lt;tf.Variable 'Variable:0' shape=(3,) dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt; Keras1234import tensorflow as tffrom tensorflow import kerasimport numpy as npimport matplotlib.pyplot as plt Load data1234mnist = keras.datasets.mnist(x_train, y_train), (x_test, y_test) = mnist.load_data()print(x_train.shape, y_train.shape)x_train, x_test = x_train / 255.0, x_test / 255.0 Model1234567model = keras.models.Sequential([ keras.layers.Flatten(input_shape=(28, 28)), keras.layers.Dense(128, activation='relu'), keras.layers.Dense(10),])print(model.summary()) Loss and optimizer1234loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)optim = keras.optimizers.Adam(lr=0.001)metrics = ['accuracy']model.compile(loss=loss, optimizer=optim, metrics=metrics) Training1234batch_size = 64epochs = 5model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, shuffle=True, verbose=2) Evaluate1model.evaluate(x_test, y_test, batch_size=batch_size, verbose=2) Predictions1234567891011121314151617probability_model = keras.models.Sequential([ model, keras.layers.Softmax()])predictions = probability_model(x_test)pred0 = predictions[0]print(pred0)'''tf.Tensor([8.67747713e-06 1.34659444e-07 6.02381151e-05 2.22242298e-03 1.12110685e-08 6.37097673e-06 6.45393591e-12 9.97610807e-01 1.71113388e-05 7.42296543e-05], shape=(10,), dtype=float32)'''label0 = np.argmax(pred0)print(label0)# 7 Another way to do predictions: model + softmax1234567891011121314predictions = model(x_test)# predictions = model.predict(x_test, batch_size = batch_size)predictions = tf.nn.softmax(predictions)pred0 = predictions[0]print(pred0)'''tf.Tensor([8.67747713e-06 1.34659444e-07 6.02381151e-05 2.22242298e-03 1.12110685e-08 6.37097673e-06 6.45393591e-12 9.97610807e-01 1.71113388e-05 7.42296543e-05], shape=(10,), dtype=float32)'''label0 = np.argmax(pred0)print(label0)# 7 Slicing123456pred05s = predictions[0:5]print(pred05s.shape)# (5, 10)label05s = np.argmax(pred05s, axis=1)print(label05s)# [7 2 1 0 4] Linear Regression12345678910111213import osos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'import matplotlib.pyplot as pltimport numpy as npimport pandas as pdnp.set_printoptions(precision=3, suppress=True)import tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import layersfrom tensorflow.keras.layers.experimental import preprocessing Load data123456url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration', 'Model Year', 'Origin']dataset = pd.read_csv(url, names=column_names, na_values='?', comment='\\t', sep=' ', skipinitialspace=True) Clean data1dataset = dataset.dropna() Convert categorical ‘Origin’ data into one-hot data1234origin = dataset.pop('Origin')dataset['USA'] = (origin == 1)*1dataset['Europe'] = (origin == 2)*1dataset['Japan'] = (origin == 3)*1 Split the data into train and test12345train_dataset = dataset.sample(frac=0.8, random_state=0)test_dataset = dataset.drop(train_dataset.index)print(dataset.shape, train_dataset.shape, test_dataset.shape)train_dataset.describe() Split features from labels12345train_features = train_dataset.copy()test_features = test_dataset.copy()train_labels = train_features.pop('MPG')test_labels = test_features.pop('MPG') Plot12345678def plot(feature, x=None, y=None): plt.figure(figsize=(10, 8)) plt.scatter(train_features[feature], train_labels, label='Data') if x is not None and y is not None: plt.plot(x, y, color='k', label='Predictions') plt.xlabel(feature) plt.ylabel('MPG') plt.legend() Normalize123456789print(train_dataset.describe().transpose()[['mean', 'std']])normalizer = preprocessing.Normalization()normalizer.adapt(np.array(train_features))print(normalizer.mean.numpy())first = np.array(train_features[:1])print('First example:', first)print('Normalized', normalizer(first).numpy()) Regression Normalize the input horsepower Apply a linear transformation (y = m*x + b) to produce 1 output using layer.Dense 12feature = 'Horsepower'single_feature","link":"/2022/11/07/TensorFlow-Tutorial/"},{"title":"Tutorial of Google Colab and PyTorch","text":"Introduction of Colab Useful Linux Commands (in Colab) Mounting Google Drive PyTorch Tutorial Outline Prerequisites What is PyTorch? PyTorch v.s. TensorFlow Overview of the DNN Training Procedure Tensor Data Type Shape of Tensor Constructor Operators Pytorch v.s. TensorFlow Device Device (GPU) How to Calculate Gradient? Overview of the DNN Training Procedure Dataset &amp; Dataloader torch.nn — Neural Network Layers torch.nn — Activation Functions torch.nn — Loss Functions torch.nn — Build your own neural network torch.optim Neural Network Training Neural Network Evaluation (Validation Set) Neural Network Evaluation (Testing Set) Save/Load a Neural Network More about PyTorch Introduction of ColabColaboratory, or “Colab” for short, allows you to write and execute Python in your browser, with $\\bullet$ Zero configuration required $\\bullet$​ Free access to GPUs $\\bullet$ Easy sharing Colab demo: https://reurl.cc/ra63jE in this demo you will learn the following: $\\bullet$ Use GPU resource provided by Google $\\bullet$ Download files using Colab $\\bullet$ Connect Google Colab with your Google drive Google Colab中可以打入代码的部分被称为code block,直接打入的代码将以Python诠释,如果前面加上感叹号!的话,程序将以shell script诠释 123import numpyimport math... 或者是 1!ls -l Exclamation mark (!) starts a new shell, does the operations, and then kills that shell, while percentage (%) affects the process associated with the notebook, and it is called a magic function. 感叹号的功能是开启一个新的shell,执行程序后将shell终止掉,如果要用cd之类的指令需要用百分比符号(%) 下面是一些常用的按键 以及保存程序的方法 Colab自带备份功能,可以通过如下方法恢复之前的文件 Useful Linux Commands (in Colab)ls: List all files in the current directory ls -l: List all files in the current directory with more detail pwd: Output the working directory mkdir &lt;dirname&gt;: Create a directory named &lt;dirname&gt; cd &lt;dirname&gt;: Move to directory named &lt;dirname&gt; gdown: Download files from google drive wget: Download files from the internet python &lt;python_file&gt;: Executes a python file Mounting Google Drive12from google.colab import drive # Import a library named google.colabdrive.mount('/content/drive', force_remount=True) # mount the content to the directory `/content/drive` PyTorch TutorialOutline$\\bullet$ Prerequisites $\\bullet$ What is PyTorch? $\\bullet$ Pytorch v.s. TensorFlow $\\bullet$ Overview of the DNN Training Procedure $\\bullet$ Tensor $\\bullet$ How to Calculate Gradient? $\\bullet$ Dataset &amp; Dataloader $\\bullet$ torch.nn $\\bullet$ torch.optim $\\bullet$ Neural Network Training/Evaluation $\\bullet$ Saving/Loading a Neural Network $\\bullet$ More About PyTorch PrerequisitesPython3 Numpy What is PyTorch?$\\bullet$​​ An open source $\\textbf{machine learning framework}$​​ $\\bullet$ A Python package that provides two high-level features: ​ $\\cdot$ $\\textbf{Tensor}$​ computation (like Numpy) with strong $\\textbf{GPU acceleration}$ ​ $\\cdot$​ Deep neural networks built on a $\\textbf{tape-based autograd}$​ system PyTorch v.s. TensorFlow Overview of the DNN Training Procedure TensorData Type Shape of Tensor Constructor$\\bullet$ From list/Numpy array 12x = torch.tensor([[1, -1], [-1, 1]])x = torch.from_numpy(np.array([[1, -1], [-1, 1]])) $\\bullet$​ Zero tensor 1x = torch.zeros([2, 2]) $2 \\times 2$矩阵,其中全部元素都是0 $\\bullet$ Unit tensor 1x= torch.ones([1, 2, 5]) $1 \\times 2 \\times 5$矩阵,其中全部元素都是1 Operators$\\bullet$ Squeeze: remove the specified dimension with length = 1 移除掉长度为1的维度 $\\bullet$ Unsqueeze: expand a new dimension 展开一个新的维度,相当于上面操作的逆操作 $\\bullet$ Transpose: transpose two specified dimensions 矩阵的转置操作 $\\bullet$​ Cat: concatenate multiple tensors 连接多个矩阵 $\\bullet$ Addition 1z = x + y $\\bullet$ Subtraction 1z = x - y $\\bullet$ Power 1y = x.pow(2) $\\bullet$​ Summation 1y = x.sum() $\\bullet$ Mean 1y = x.mean() Pytorch v.s. TensorFlow$\\bullet$ Shape manipulation Device$\\bullet$ Default: tensor &amp; modules will be computed with CPU $\\bullet$ CPU 1x = x.to('cpu') $\\bullet$GPU 1x = x.to('cuda') Device (GPU)$\\bullet$ Check if your computer has NVDIA GPU 1torch.cuda.is_available() $\\bullet$ Multiple GPUs: specify ‘cuda:0’, ‘cuda:1’, ‘cuda:2’, … $\\bullet$ Why GPU? ​ $\\cdot$ Parallel computing ​ $\\cdot$ https://towardsdatascience.com/what-is-a-gpu-and-do-you-need-one-in-deep-learning-718b9597aa0d How to Calculate Gradient? Overview of the DNN Training Procedure Dataset &amp; Dataloader 1234567891011from torch.utils.data import Dataset, DataLoaderclass MyDataset(Dataset): def __init__(self, file): self.data = ... def __getitem__(self, index): return self.data[index] def __len__(self): return len(self.data) 12dataset = MyDataset(file)dataloader = DataLoader(dataset, batch_size, shuffle=True) 对于Training data,我们需要shuffle,对于Testing data,则不需要shuffle torch.nn — Neural Network Layers Linear Layer (Fully-connected Layer) 1nn.Linear(in_features, out_features) in_features: 输入input的维度 out_features: 输出output的维度 比如这样一个网络 前后输入矩阵*部分可以是任何大小 12ayer = torch.nn.Linear(32, 64)layer.weight.shape torch.Size([64, 32]) 显示W矩阵的形状 1layer.bias.shape torch.Size([64]) torch.nn — Activation Functions Sigmoid Activation reLU Activation torch.nn — Loss Functions Mean Squared Error (for linear regression) 1nn.MSELoss() Cross Entropy (for classification) 1nn.CrossEntropyLoss() torch.nn — Build your own neural network12345678910111213import torch.nn as nnclass MyModel(nn.Module): def __init__(self): super(MyModel, self).__init__() self.net = nn.Sequential( nn.Linear(10, 32) nn.Sigmoid(), nn.Linear(32, 1) ) def forward(self, x): return self.net(x) 这样就建立了一个如上图所示的neural network torch.optim Optimization algorithms for neural networks (gradient descent) Stochastic Gradient Descent (SGD) 1torch.optim.SGD(params, lr, momentum = 0) params是需要输入model的parameter: model.parameters() Neural Network Training12345678910111213141516device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # 单GPU或者CPUdataset = MyDataset(file) # read data via MyDatasettr_set = DataLoader(dataset, 16, shuffle = True) # put dataset into Dataloadermodel = MyModel().to(device) # contruct model and move to device (CPU/CUDA)creterion = nn.MSELoss() # set loss functionoptimizer = torch.optm.SGD(model.parameters(),0.1) # set optimizerfor epoch in range(n_epochs): # iterate n_epochs model.train() # set model to train mode for x, y in tr_set: # iterate through the dataloader optimizer.zero_grad() # set gradient to zero x, y = x.to(device), y.to(device) # move data to device pred = model(x) # forward pass (compute output) loss = criterion(pred, y) # compute loss loss.backward() # compute gradient (backpropagation) optimizer.step() # update model with optimizer Neural Network Evaluation (Validation Set)123456789model.eval() # set model to evaluation modetotal_loss = 0for x,y in dv_set: # iterate through the dataloader x, y = x.to(device), y.to(device) # move data to device with torch.no_grad(): # disable gradient caculation pred = model(x) # forward pass (compute output) loss = criterion(pred, y) # compute loss total_loss += loss.cpu().item * len(x) # accumulate loss avg_loss = total_loss / len(dv_set.dataset) # compute averaged loss Neural Network Evaluation (Testing Set)1234567model.eval() # set model to evaluation modepreds = []for x in tt_set: # iterate through the dataloader x = x.to(device) # move data to device with torch.no_grad(): # disable gradient caculation pred = model(x) # forward pass (compute output) preds.append(pred.cpu()) # collect prediction Save/Load a Neural Network Save 1torch.save(model.state_dict(), path) Load 12ckpt = torch.load(path)model.load_state_dict(ckpt) More about PyTorch torchaudio speech/audio processing torchtext natural language processing torchvision computer vision skorch scikit-learn + pyTorch 更多详细信息可以去看PyTorch的官方文档","link":"/2021/09/17/Totorial-of-Google-Colab-and-Pytorch/"},{"title":"Use GAN to Generate Anime Style Pictures(to Be Continued)","text":"March.14.2020- 1.Web Crawler for Pixiv findPID.py findFollow.py main.py 2.Web Crawler for Konachan 3.Anime Face Recognition and Capture 4.Train 1.Web Crawler for Pixiv https://www.pixiv.net is a Japanese online community for artists. As of September 2016, the site consists of over 20 million members, over 43 million submissions, and receives over 3.7 billion page views monthly, Which means it is a great database for our project. But as we can see, the quality of these pictures is uneven. To filter out the poor quality Illustrations, we have several alternatives to solve that. We chose the easiest way: By only crawling the illustrators I followed. I chose to use python to make a web crawler. My web crawler project has three programs, main.py, findPID.py, findFollow.py. We will discuss these programs in a bit more detail. findPID.pyIt includes serval ways to find an illustration’s ID (each illustration has a unique ID, we need it to download the picture file). 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328import reimport urllib.request,urllib.error,urllib.parseimport gzipfindID = re.compile(r'&quot;(\\d+?)&quot;:null')findDailyID = re.compile(r'&lt;a href=&quot;/artworks/(\\d+?)&quot;class=&quot;t')findTagID = re.compile(r'&quot;id&quot;:&quot;(\\d+?)&quot;,&quot;title&quot;:')def findPIDs_by_Author(pid): baseurls = &quot;https://www.pixiv.net/ajax/user/&quot; baseurle = &quot;/profile/all?lang=zh&quot; PIDs = [] url = baseurls + str(pid) + baseurle res = askUrl(url) html = res.read() # print(html) # print(res) # soup = BeautifulSoup(html, 'html.parser') # html = str(html) # print(html) ret = gzip.decompress(html).decode(&quot;utf-8&quot;) # print(ret) # print(re.findall(findID,ret)) PIDs = re.findall(findID,ret) print(PIDs) print(len(PIDs)) return PIDsdef findPIDs_by_Daily(): url = 'https://www.pixiv.net/ranking.php' res = askUrl(url) html = res.read() ret = gzip.decompress(html).decode(&quot;utf-8&quot;) # print(ret) PIDs = re.findall(findDailyID,ret) print(PIDs) print(len(PIDs)) return PIDsdef findPIDs_by_Daily_R18(): url = 'https://www.pixiv.net/ranking.php?mode=daily_r18' res = askUrl(url) html = res.read() ret = gzip.decompress(html).decode(&quot;utf-8&quot;) # print(ret) PIDs = re.findall(findDailyID,ret) print(PIDs) print(len(PIDs)) return PIDsdef findPIDs_by_Weekly(): url = 'https://www.pixiv.net/ranking.php?mode=weekly' res = askUrl(url) html = res.read() ret = gzip.decompress(html).decode(&quot;utf-8&quot;) # print(ret) PIDs = re.findall(findDailyID,ret) print(PIDs) print(len(PIDs)) return PIDsdef findPIDs_by_Weekly_R18(): url = 'https://www.pixiv.net/ranking.php?mode=weekly_r18' res = askUrl(url) html = res.read() ret = gzip.decompress(html).decode(&quot;utf-8&quot;) # print(ret) PIDs = re.findall(findDailyID,ret) print(PIDs) print(len(PIDs)) return PIDsdef findPIDs_by_Male(): url = 'https://www.pixiv.net/ranking.php?mode=male' res = askUrl(url) html = res.read() ret = gzip.decompress(html).decode(&quot;utf-8&quot;) # print(ret) PIDs = re.findall(findDailyID,ret) print(PIDs) print(len(PIDs)) return PIDsdef findPIDs_by_Male_R18(): url = 'https://www.pixiv.net/ranking.php?mode=male_r18' res = askUrl(url) html = res.read() ret = gzip.decompress(html).decode(&quot;utf-8&quot;) # print(ret) PIDs = re.findall(findDailyID,ret) print(PIDs) print(len(PIDs)) return PIDsdef findPIDs_by_Female(): url = 'https://www.pixiv.net/ranking.php?mode=female' res = askUrl(url) html = res.read() ret = gzip.decompress(html).decode(&quot;utf-8&quot;) # print(ret) PIDs = re.findall(findDailyID,ret) print(PIDs) print(len(PIDs)) return PIDsdef findPIDs_by_Female_R18(): url = 'https://www.pixiv.net/ranking.php?mode=female_r18' res = askUrl(url) html = res.read() ret = gzip.decompress(html).decode(&quot;utf-8&quot;) # print(ret) PIDs = re.findall(findDailyID,ret) print(PIDs) print(len(PIDs)) return PIDsdef findPIDs_by_Monthly(): url = 'https://www.pixiv.net/ranking.php?mode=monthly' res = askUrl(url) html = res.read() ret = gzip.decompress(html).decode(&quot;utf-8&quot;) # print(ret) PIDs = re.findall(findDailyID,ret) print(PIDs) print(len(PIDs)) return PIDsdef findPIDs_by_Tag(): tagn = int(input('Please enter number of tag:')) mode = input('Please enter search method:\\n1.in strict accordance with tag\\n2.Keywords can longer than tag\\n') if mode == '1': urle = '&amp;s_mode=s_tag_full&amp;type=all&amp;lang=zh' elif mode == '2': urle = '&amp;s_mode=s_tag&amp;type=all&amp;lang=zh' else: print('Invalid input, default method 2') urle = '&amp;s_mode=s_tag&amp;type=all&amp;lang=zh' sumtag1 = '' sumtag2 = '' sumtag3 = '' # urle = '&amp;s_mode=s_tag&amp;type=all&amp;lang=zh' count = 0 for n in range(0,tagn): tag = input('Please enter tag:') if tag.find('(') &lt; 0: urltag = urllib.parse.quote(tag) # urle = '&amp;s_mode=s_tag_full&amp;type=all&amp;lang=zh' if count &gt; 0: count = count + 1 sumtag1 = sumtag1 + r'%20' + urltag sumtag2 = sumtag1 sumtag3 = sumtag3 + '_' + tag else: count = count + 1 sumtag1 = urltag sumtag2 = sumtag1 sumtag3 = tag else: #如果是带括号的tag print('Tag with brackets') # urle = '&amp;s_mode=s_tag&amp;type=all&amp;lang=zh' tag1 = re.split('\\(' ,tag) tag1[0] = urllib.parse.quote(tag1[0]) tag2 = re.split('\\)' ,tag1[1]) tag2[0] = urllib.parse.quote(tag2[0]) tag2[1] = urllib.parse.quote(tag2[1]) if count &gt; 0: count = count + 1 sumtag2 = sumtag2 + r'%20' + urllib.parse.quote(tag) sumtag1 = sumtag1 + r'%20' + tag1[0] + '(' + tag2[0] + ')' + tag2[1] sumtag3 = sumtag3 + '_' + tag else: count = count + 1 # print(urllib.parse.quote(tag)) # print(tag) sumtag2 = urllib.parse.quote(tag) sumtag1 = tag1[0] + '(' + tag2[0] + ')' + tag2[1] sumtag3 = tag # print(sumtag2) # print(sumtag1) sumtag3 = sumtag3 + '_mode' + mode urls = 'https://www.pixiv.net/ajax/search/artworks/' urlm1 = '?word=' urlm2 = '&amp;order=date_d&amp;mode=all&amp;p=' # urle = '&amp;s_mode=s_tag_full&amp;type=all&amp;lang=zh' # tag = input('请输入要查找的tag:') p = input('Please enter the page you need to download:') url = urls + sumtag1 + urlm1 + sumtag2 + urlm2 + p +urle # print(sumtag2) # print(url) res = askUrl(url) html = res.read() ret = gzip.decompress(html).decode(&quot;utf-8&quot;) # print(ret) PIDs = re.findall(findTagID, ret) print(PIDs) print(len(PIDs)) PIDs.append(sumtag3) return PIDsdef findPIDs_by_Tag_sort(): tagn = int(input('Please enter number of tag:')) mode = input('Please enter search method:\\n1.in strict accordance with tag\\n2.Keywords can longer than tag\\n') if mode == '1': urle = '&amp;s_mode=s_tag_full&amp;type=all&amp;lang=zh' elif mode == '2': urle = '&amp;s_mode=s_tag&amp;type=all&amp;lang=zh' else: print('Invalid input, default method 2') urle = '&amp;s_mode=s_tag&amp;type=all&amp;lang=zh' sumtag1 = '' sumtag2 = '' sumtag3 = '' # urle = '&amp;s_mode=s_tag&amp;type=all&amp;lang=zh' count = 0 for n in range(0,tagn): tag = input('Please enter tag:') if tag.find('(') &lt; 0: urltag = urllib.parse.quote(tag) # urle = '&amp;s_mode=s_tag_full&amp;type=all&amp;lang=zh' if count &gt; 0: count = count + 1 sumtag1 = sumtag1 + r'%20' + urltag sumtag2 = sumtag1 sumtag3 = sumtag3 + '_' + tag else: count = count + 1 sumtag1 = urltag sumtag2 = sumtag1 sumtag3 = tag else: #如果是带括号的tag print('Tag with brackets') # urle = '&amp;s_mode=s_tag&amp;type=all&amp;lang=zh' tag1 = re.split('\\(' ,tag) tag1[0] = urllib.parse.quote(tag1[0]) tag2 = re.split('\\)' ,tag1[1]) tag2[0] = urllib.parse.quote(tag2[0]) tag2[1] = urllib.parse.quote(tag2[1]) if count &gt; 0: count = count + 1 sumtag2 = sumtag2 + r'%20' + urllib.parse.quote(tag) sumtag1 = sumtag1 + r'%20' + tag1[0] + '(' + tag2[0] + ')' + tag2[1] sumtag3 = sumtag3 + '_' + tag else: count = count + 1 # print(urllib.parse.quote(tag)) # print(tag) sumtag2 = urllib.parse.quote(tag) sumtag1 = tag1[0] + '(' + tag2[0] + ')' + tag2[1] sumtag3 = tag # print(sumtag2) # print(sumtag1) sumtag3 = sumtag3 + '_mode' + mode urls = 'https://www.pixiv.net/ajax/search/artworks/' urlm1 = '?word=' urlm2 = '&amp;order=date_d&amp;mode=all&amp;p=' # urle = '&amp;s_mode=s_tag_full&amp;type=all&amp;lang=zh' # tag = input('请输入要查找的tag:') p = 1 PIDs = [] while 1: print('page = %d' %p) url = urls + sumtag1 + urlm1 + sumtag2 + urlm2 + str(p) + urle res = askUrl(url) html = res.read() ret = gzip.decompress(html).decode(&quot;utf-8&quot;) AddPIDs = re.findall(findTagID, ret) PIDs = PIDs + AddPIDs if len(AddPIDs) &lt; 60 or p &gt; 999: break p = p + 1 PIDs.append(sumtag3) # p = input('Please enter the page you need to download:') # url = urls + sumtag1 + urlm1 + sumtag2 + urlm2 + p +urle # # print(sumtag2) # # print(url) # res = askUrl(url) # html = res.read() # ret = gzip.decompress(html).decode(&quot;utf-8&quot;) # # print(ret) # PIDs = re.findall(findTagID, ret) # print(PIDs) # print(len(PIDs)) # PIDs.append(sumtag3) return PIDsdef askUrl(url): head = { #for security I delete my header } request = urllib.request.Request(url, headers=head) html = '' try: response = urllib.request.urlopen(request) # html = response.read() # print(response) # print(html) return response except urllib.error.URLError as e: if hasattr(e, &quot;code&quot;): print(e.code) if hasattr(e, &quot;reason&quot;): print(e.reason)# findPIDs_by_Author(4338012)# findPIDs_by_Daily()# findPIDs_by_Daily_R18()# findPIDs_by_Weekly()# findPIDs_by_Weekly_R18()# findPIDs_by_Male()# findPIDs_by_Male_R18()# findPIDs_by_Female()# findPIDs_by_Female_R18()# findPIDs_by_Monthly()# findPIDs_by_Tag() findFollow.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import urllib.request,urllib.errorimport getInfoimport mathimport refindUID = re.compile(r'\\{&quot;userId&quot;:&quot;(\\d+?)&quot;,')def findFollow(mode): urls = 'https://www.pixiv.net/ajax/user/14937409/following?offset=' urlm1 = '&amp;limit=100' urlm2 = '&amp;rest=' urle = '&amp;tag=&amp;lang=zh' follows = [] if mode == 1: smode = 'hide' follow = getInfo.getHideFollow() else: smode = 'show' follow = getInfo.getShowFollow() for i in range(0,math.ceil(follow/100)): num = i*100 url = urls + str(num) + urlm1 + urlm2 + smode + urle res = askUrl(url) html = str(res.read()) # print(res.read()) # print(html) fos = re.findall(findUID,html) # print(fos) # print(len(fos)) follows.extend(fos) # print(follows) # print(len(follows)) return followsdef askUrl(url): head = { #for security I delete my header } request = urllib.request.Request(url, headers=head) html = '' try: response = urllib.request.urlopen(request) # html = response.read() # print(response) # print(html) return response except urllib.error.URLError as e: if hasattr(e, &quot;code&quot;): print(e.code) if hasattr(e, &quot;reason&quot;): print(e.reason) pass# findFollow(2) main.py2.Web Crawler for Konachan12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import requests#from bs4 import BeautifulSoupimport osimport tracebackimport rebasePath = r'/usr/share/nginx/od/ML/'findImg = re.compile(r'&quot;directlink largeimg&quot; href=&quot;(.+?)&quot;')findID = re.compile(r'Post.register\\({&quot;id&quot;:(.+?),')def download(url, filename): if os.path.exists(filename): print('file exists!') return try: r = requests.get(url, stream=True, timeout=600) r.raise_for_status() with open(filename, 'wb') as f: for chunk in r.iter_content(chunk_size=1024): if chunk: # filter out keep-alive new chunks f.write(chunk) f.flush() return filename except KeyboardInterrupt: if os.path.exists(filename): os.remove(filename) raise KeyboardInterrupt except Exception: traceback.print_exc() if os.path.exists(filename): os.remove(filename)os.chdir(basePath)if os.path.exists('Konachan') is False: os.makedirs('Konachan')path = basePath + 'Konachan/'os.chdir(path)start = 1end = 20for i in range(start, end + 1): url = 'http://konachan.net/post?page=%d&amp;tags=' % i html = requests.get(url).text # soup = BeautifulSoup(html, 'html.parser') # print(html) imgs = re.findall(findImg,html) ids = re.findall(findID,html) # print(imgs) count = 0 for img in imgs: # target_url = 'http:' + img['src'] target_url = img # print(target_url) id = ids[count] count = count + 1 # filename = os.path.join('Konachan', target_url.split('/')[-1]) filename = id + '.jpg' download(target_url, filename) print('%d / %d' % (i, end)) 3.Anime Face Recognition and CaptureBased on a project on github: https://github.com/nagadomi/lbpcascade_animeface 4.Train","link":"/2021/03/23/Use-GAN-to-Generate-Anime-Style-Pictures-to-Be-Continued/"},{"title":"Hello World","text":"This is my first blog. Just for testing.","link":"/2021/04/15/hello-world/"},{"title":"Notes of Machine Learning(P1-P11)","text":"Machine LearningYuzi Liang 1.Regression Step 1:Model Step 2: Goodness of function Step 3: Best Function Step 4: Gradient Descent Adaptive Learning Rates Adagrad Stochastic Gradient Descent Feature Scaling 2.Optimization for Deep Learning SGD with Momentum(SGDM) RMSProp Adam Optimizer: Real Application Adam vs SGDM 3.Classification: Probabilistic Generative Model How to do Classfication Ideal Alternatives Generative Model Two Classes Three Steps Function Set(Model) Goodness of a function Find the best function: easy Logistic Regression Function Set Goodness of a Function Find the Best Function Discriminative v.s. Generative Multi-class Classification 1.RegressionStep 1:ModelChoose a model to generate results. E.g. Linear model y = b +\\sum w_i x_i$x_i$: feature, $w_i$: weight, $b$: bias Step 2: Goodness of functionvalue by the Loss Function. L(f) = \\sum^N_{n=1} (\\hat y^n - f(x^n))^2Loss Function is a function of function. Regularization y = b + \\sum w_i x_i L = \\sum_n(\\hat y - (b + \\sum w_i x_i))^2 + \\lambda \\sum(w_i)^2Larger $\\lambda$ means smoother curve. Step 3: Best FunctionPick the best function. f^* = arg \\ \\textrm{min} \\ L(f)$ arg $ $\\textrm{min}$: argument of the minimum, that is to say, the set of points(functions) of the given for which the value of the given expression attains its minimum value. Step 4: Gradient DescentHow to find the Best Function. Still use the example of linear model, where the $L$ is the function of $w$: w^* = arg \\ \\min_w \\ L(w)choose a value $w_0$: w^1 \\leftarrow w^0 - \\eta \\frac{dL}{dw}|_{w = w^0} w^2 \\leftarrow w^1 - \\eta \\frac{dL}{dw}|_{w = w^1}$\\eta$ is called “learning rate” Q: How about two parameters? A: Use gradient. \\vec{w}^{i+1} = \\vec{w}^i - \\eta \\nabla LSome tips: Adaptive Learning RatesPopular &amp; Simple Idea: Reduce the learning rate by some factor every few epochs E.g. $1/t \\ decay: \\eta^t = \\eta/\\sqrt{t+1} $ Learning rate cannot be one-size-fits-all AdagradDivide the learning rate of each parameter by the root mean square of its previous derivatives. w^{t+1} \\leftarrow w^t -\\frac{\\eta^t}{\\sigma^t}g^t \\sigma^t = \\sqrt{\\frac{1}{t+1}\\sum^t_{i=0}(g^i)^2}$g^t = \\frac{\\partial L(\\theta^t)}{\\partial w}$, noticed that $\\eta^t,\\sigma^t$ both have factor $1/\\sqrt{t+1}$ w^{t+1} \\leftarrow w^t - \\frac{\\eta}{\\sqrt{\\sum^t_{i=0}(g^i)^2}}g^tStochastic Gradient DescentWhen use gradient descent: L = \\sum_n(\\hat{y}^n-(b+\\sum_iw_ix^n_i))^2 \\theta^i = \\theta^{i-1} - \\eta \\nabla L(\\theta^{i-1})Stochastic Gradient Descent is to say: Pick an example $x^n$(random or sequential) L = (\\hat{y}^n-(b+\\sum_iw_ix^n_i))^2faster than gradient descent when there are many examples. Feature ScalingE.g. y = b + w_1x_1 + w_2x_2sometimes $x_1 \\ll x_2$, then $L$ will be smooth in $w_1$ direction, sharp in $w_2$ direction. We need to rescale $x_1,x_2$. Usually we use: x_i^r \\leftarrow \\frac{x_i^r-m_i}{\\sigma_i}$m_i$: mean, $\\sigma_i$: standard deviation 2.Optimization for Deep LearningSGD with Momentum(SGDM)Start at: point $\\theta^0$, movement $v^0 = 0$, use $ v $ to represent the momentum. Next step: $v^1 = \\lambda v^0 - \\eta \\nabla L(\\theta^0)$, move to $\\theta^1 = \\theta^0 + v^1$ Next step: $v^2 = \\lambda v^1 - \\eta \\nabla L(\\theta^1)$, move to $\\theta^2 = \\theta^1 + v^2$ …… Movement not just based on gradient, but previous movement. RMSProp \\theta_t = \\theta_{t-1}-\\frac{\\eta}{\\sqrt{v_t}}(g_{t-1})^2 v_1 = g^2_0 v_t = \\alpha v_{t-1} + (1-\\alpha)(g_{t-1})^2so that $v_t$ will not increase constantly. AdamWe have SGDM: \\theta_t = \\theta_{t-1} - \\eta m_t m_t = \\beta_1 m_{t-1} + (1-\\beta_1)g_{t-1}RMSProp: \\theta_t = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{v_t}}g_{t-1} v_1 = g_0^2 v_t = \\beta_2 v_{t-1} + (1-\\beta_2)(g_{t-1})^2Combine SGDM with RMSProp, we get Adam: \\theta_t = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v_t}}+\\epsilon}\\hat{m_t} \\hat{m_t} = \\frac{m_t}{1-\\beta^t_1} v_t = \\frac{v_t}{1-\\beta_2^t}The collective name of Adagrad, RMSProp and Adam is Adaptive learning rate. Optimizer: Real ApplicationE.g. BERT, Transformer, Tacotron(Speech Synthesis), YOLO(CV), Mask R-CNN, ResNet, Big-GAN, MAML(Classfication) Adam vs SGDMAdam: fast training, large generalization gap, unstable. SGDM: stable, little generation gap, better convergence(?) Can we simply combine Adam with SGDM? We have SWATS: Begin with Adam(fast), end with SGDM. But we need to consider the change criteria and we can not get a strict criteria. (The TA is making me very sleepy, I have to skip this class for the moment) 3.Classification: Probabilistic Generative ModelHow to do ClassficationIf we simply use linear model(Regression): We may get the purple line instead of the green line because of the fitting need the result reach the minimal. Ideal AlternativesFunction(Model): x \\rightarrow \\left\\{ \\begin{aligned} & g(x)>0 & & \\rm{Output} = \\rm{Class1} \\\\ & else & & \\rm{Output} = \\rm{Class2} \\end{aligned} \\right.Loss function: L(f) = \\sum_n \\delta(f(x^n) \\neq \\hat{y}^n)means the number of times $f$ get incorrect results on training data. Since L is not differentiable, we can not use Gradient Descent. It seems like that we can use Perception or SVM, but we will not talk about it today. Generative ModelTwo ClassesClass1:$P(C_1) \\ , \\ P(x|C_1)$ Class2:$P(C_2) \\ , \\ P(x|C_2)$ We have: P(C_1|x) = \\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}Generative Model: $P(x) = P(x|C_1)P(C_1)+P(x|C_2)P(C_2)$ $x$的分布取最大似然估计. 若取$x$分布为高斯分布,大部分情况下不会分别讨论每一个类的方差，而是考虑一个统一的covariance matrix. 综合方差的计算 $\\Sigma = $权重 $\\times \\Sigma^i$的求和.权重就是数据量之比. Three StepsFunction Set(Model)parameters of model: $P(C_1) \\ , \\ P(x|C_1) \\ , \\ P(C_2) \\ , \\ P(x|C_2)$ P(C_1|x) = \\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}if $P(C_1|x)&gt;0.5$, output: class 1 Otherwise, output: class 2 P(C_1|x) = \\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)} \\\\ = \\frac{1}{1+\\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}} = \\frac{1}{1+exp(-z)} = \\sigma(z)其中: z = ln\\frac{P(x|C_1)P(C_1)}{P(x|C_2)P(C_2)}$\\sigma(z)$的图像如下: 若取统一的方差,经过化简可以得到: z = w \\cdot x + b = \\sum_i w_i x_i + b分界线就变成了一条直线(或者是$n-1$维超平面) Goodness of a functionFind the best function: easyLogistic RegressionFunction Set f_{w,b}(x) = \\sigma(\\sum_i w_i x_i + b)Goodness of a FunctionCross entropy: C(f(x^n),\\hat{y}^n) = -[\\hat{y}^nlnf(x^n)+(1-\\hat{y}^n)ln(1-f(x^n))]其中: \\hat y^n: 1 \\ \\rm{for \\ class} \\ 1, \\ 0 \\ \\rm{for \\ class} \\ 2需要使交叉熵之和最小,即 L(f) = \\sum_n C(f(x^n),\\hat y^n)最小. Find the Best Function求微分,用 Gradient Descent \\frac{\\partial L(w,b)}{\\partial w_i} = \\sum_n -(\\hat y^n - f_{w,b}(x^n))x_i^n后面讨论了选取交叉熵而不是像之前一样选择方差的原因:若选择方差,在远离最优解的地方导数也很小,导致不好选取步长 Discriminative v.s. Generative 这两种方法寻找到的结果是不一样的.一般情况下前者效果更好,Generative有额外假设,受data影响较小. Multi-class Classification3 classes as example C_1:w^1,b_1 \\ \\ \\ z_1 = w^1 \\cdot x + b_1\\\\ C_2:w^2,b_2 \\ \\ \\ z_2 = w^2 \\cdot x + b_2\\\\ C_3:w^3,b_3 \\ \\ \\ z_3 = w^3 \\cdot x + b_3 随后使交叉熵每个分量最小即可. Limitation of Logistic Regression 有些分类没法做,但是可以利用Feature Transformation(有些Heuristic Ad Hoc 启发性和临时性的东西)解决. 从而衍生出来了级联逻辑回归模型. 进入Deep Learning.","link":"/2021/03/23/notes-of-machine-learning/"},{"title":"Stock Price Forecast Based on Monte Carlo Method and Machine Learning","text":"A project for Computational Physics with my roommate Xinxiang You. February.20.2020-March.24.2020 1.Why We Choose This Topic 2.Algorithm Introduction 1.Get Stock Data 2.Data Processing 3.Deploy to Server Installation of Python Installation of Nginx 3.Result presentation code = ‘600837.ss’ , M = 1000 , N =100 code = ‘600519.ss’ , M = 1000 , N =100 4.LSTM code = ‘600519.ss’ , epochs = 300 1.Why We Choose This TopicIn 2020, many young people begin to enter the stock, fund market, most have not studied the relevant professional knowledge. These choices based on chance and luck are often unreliable, so we’re going to use computational physics to simulate the future stock price roughly, to provide reference. We also rent Ali ECS cloud server to implement a web API, trying to achieve the function of user interaction, Which means we can provide the server for every user. The program are mostly implemented with Python. 2.Algorithm Introduction1.Get Stock DataWe use the yahoo API to get the previous stock data according to the stock code, the function is: 1data_csvsave = web.DataReader('600837.ss', 'yahoo', datetime.datetime(2019,1,1), datetime.date.today()) The first parameter 600837.ss is the stock code of Haitong Securities, we can easily get any other stock data by changing the input of stock code. The second parameter means we use the API of yahoo. The third and fourth parameters specify the acquisition date is from 2019.1.1 to today. 2.Data ProcessingThe specific formula of Monte Carlo simulation is: \\frac{\\Delta S}{S}=\\mu \\Delta t +\\sigma \\epsilon \\sqrt{\\Delta t}$S$ is the initial stock price, $\\Delta S$ is the change of the initial price, $\\mu$ is expected yield, $\\sigma$ is stock risk, can be replaced by the standard deviation. All of them can calculate from history data of the stock. $\\epsilon$ is a random variable that satisfy a normal distribution in [0,1]. Stock price of the $N$th day Can be predicted according to the formula below: S_{N}=S_{0}+S_0 (\\mu N+\\sigma \\epsilon \\sqrt{N} )In the real simulation, we use parameter $M$ to represent the number of simulation, Simply Use the average of these simulation result to predict the stock price of $N$th day. The key function is down below, start_price represent to the stock price of today, drift is $S_0 \\mu N$, shock is $S_0 \\sigma \\epsilon \\sqrt{N}$, $N$ is the number of days. The final results are saved in the list data_new[]. 12345678910111213141516def Mon_carluo(start_price,drift,shock,N):data_new=[]pi=start_pricedata_new.append(pi)for i in range(1,N): a=drift+random.normal(drift,shock) if a&gt;0.1: #Chinese stocks are not allowed to rise or fall more than 10% pi=pi+pi*0.1 data_new.append(pi) elif a&lt;-0.1: pi=pi-pi*0.1 data_new.append(pi) else: pi=pi+pi*a data_new.append(pi) return data_new Improvement of algorithm: General when absolute value of yield is larger, the yield over the next few days will be a big up and downs, which means the past few days had a greater influence on absolute value of yield, and the yield has little to do with the stock price long ago. We added the predicted data to historical data to make predictions, and fixed the number of historical data unchanged. The code is shown below: 123456789101112131415161718192021def Mon_carluo(data,M,LEN): #Monte Carlo simulation for the next one day, number of simulation is M, return list of yield sum=0 for i in range(0,M): if pan_duan(data): dt = 1/DAYS sigma=np.std(data[len(data)-7:len(data)],ddof=1)#sigma: standard deviation, replace of stock risk mu=np.mean(data[len(data)-7:len(data)]) shock=sigma*np.sqrt(dt) drift=mu*dt a=drift+random.normal(drift,shock) else: days=len(data) dt = 1/LEN sigma=np.std(data[len(data)-LEN:len(data)],ddof=1)#sigma: standard deviation, replace of stock risk mu=np.mean(data[len(data)-LEN:len(data)]) shock=sigma*np.sqrt(dt) drift=mu*dt a=drift+random.normal(drift,shock) sum+=adata.append(sum/M)return data 3.Deploy to ServerThis part is for display of result, to do that, we rent an Ali ECS server, the operating system is CentOS7, public network IP is: 47.117.126.163 Then we can step into the configuration of server. Installation of PythonUsing the command yum to install python. Before that we need to install some prepositive libraries. 12yum -y groupinstallyum -y install zlib-devel Then visit python official website https://www.python.org/, Download the appropriate Linux version. We downloaded Python-3.6.8.tgz, and uploaded it to our server. 12345tar -xvzf Python-3.6.8.tgzcd Python-3.6.8/./configure --prefix=/usr/pythonmakemake install Installation complete. Installation of NginxSynchronous display on a web page is implemented by nginx. Installation of prepositive libraries: 123yum -y install gcc-c++yum install -y pcre pcre-develyum install -y openssl openssl-devel Using the command wget to download nginx: 1wget -c https://nginx.org/download/nginx-1.12.0.tar.gz Unzip and install: 1234tar -zxvf nginx-1.12.0.tar.gzcd nginx-1.12.0makemake install Start nginx: 1systemctl start nginx Then configure HTML files to achieve function of real-time display. 3.Result presentationYou can use it on http://47.117.126.163:5000/result by yourself. code = ‘600837.ss’ , M = 1000 , N =100 code = ‘600519.ss’ , M = 1000 , N =100 4.LSTMThe predictions above are not very precise, so we turned to the machine learning. Long short-term memory (LSTM) is an artificial neural network (RNN) architecture used in the field of deep learning. It’s good at handling stock forecasts. We can see that the results are very precise. But the calculation needs grate computing power, so it’s not suitable for deployment on the server since the ECS server even does not have a GPU. So we just run the program on our own computer. code = ‘600519.ss’ , epochs = 300 left: data set/train set = 1/1, data from 1/1/2019 to 3/21/2021right: data set/train set = 4/1, data from 1/1/2019 to 3/21/2021 left: data set/train set = 1/1, data from 1/1/2018 to 3/21/2021right: data set/train set = 4/1, data from 1/1/2018 to 3/21/2021","link":"/2021/03/23/stock-price-forecast-based-on-Monte-Carlo-method-and-machine-learning/"},{"title":"Synthesis and Measurement of $CrSbSe_3$(to Be Continued)","text":"March.19.2020- Synthesis and Measurement of $CrSbSe_3$ Previous Work 4.17.2021 4.20.2021 5.09.2021 EDS 5.12.2021 Raman Spectroscopy(532nm) References Synthesis and Measurement of $CrSbSe_3$Previous WorkLast semester I tried to synthesis $BiCrSe$ misfit material, but after I measured the polycrystal. I found that the results are not very good. The results are as follows. $\\ $So we gave up on further research on this material. 4.17.2021According to these articles, there are two ways to get monocrystalline $CrSbSe_3$: 4.20.2021Because in our laboratory we can not centrifuge in a high temperature, we ignore the last several steps: ‘Molten liquid was separated from the crystals in a centrifuge, with silica wool serving as a filter. Remaining flux that attached to the surface of crystals was removed by keeping the crystal at 500 ◦C for 3 days in a sealed silica tube while leaving the cold end of the tube at room temperature.’ Then we set up the calcination program, waiting for 5~6 days. 5.09.2021Acicular single crystal samples with a diameter on the order of micron were found. EDS Had an EDS analysis.Cr:Se:Sb = 1:2:1 not the expected 1:3:1 5.12.2021Raman Spectroscopy(532nm)Two points were measured. The unit of x-axis is nanometers. ReferencesAnisotropic magnetocaloric effect and critical behavior in $CrSbSe_3$ Origin of Anisotropic Magnetic Properties in the Ferromagnetic Semiconductor $CrSbSe_3$ with a Pseudo-One-Dimensional Structure Synthesis and Structure of CrSbSes: A Pseudo-One-Dimensional Ferromagnet Anisotropic magnetocaloric effect and critical behavior in $CrSbSe_3$","link":"/2021/03/29/synthesis-and-measurement-of-CrSbSe-3/"},{"title":"Image Generator: Application and Implementation of StyleGAN","text":"Image Generator: Application and Implementation of StyleGAN 一 项目简介 二 项目目标 1.项目目标 2.基本原理与模型 2.1 极大似然估计 2.2 GAN的基本原理 2.3 StyleGAN对于人脸生成的优化 三 项目具体实现 1.爬虫 1.1 Pixiv 1.1.1 fetInfo.py 1.1.2 findPID.py 1.1.3 findFollow.py 1.1.4 main.py 1.2 Konachan 2.人脸提取 3.部署环境运行代码 第一台电脑上的环境部署 第二台电脑上的环境部署 四 结果展示 五 关于GAN的进一步讨论 GAN优点 GAN缺点 GAN应用 GAN引起的担忧（危害） 六 关于本项目的总结 Image Generator: Application and Implementation of StyleGAN一 项目简介​ 深度学习是近十年来人工智能领域取得的最重要的突破之一。它在语音识别、自然语言处理、计算机视觉、图像与视频分析、多媒体等诸多领域都取得了巨大成功。而其中，生成式对抗网络（GAN,Generative Adversarial Networks）这一深度学习模型是近年来复杂分布上无监督学习最具前景的方法之一，广泛应用于图像生成和图像识别的领域中，如神经风格迁移、Google公司开发的Deep Dream算法和变分自编码器。 ​ 最近在动漫绘画领域当中，也开始有人尝试用机器学习来制作2D动漫图像，从而代替传统的手绘和三渲二这些较为耗人力、制作周期长的方法。cinnamon AI开始提供动漫自动着色功能，可以进行一定的补齐线稿缺线的预处理以及自动上色的功能，将对单元图像进行着色所需时间缩短至1/10，并且将成本降低了50%以上，一定程度上减少画师的工作量，提高动漫创作者的生产力；ATD-12K是一个大规模的动画的三元组帧间数据集，可以在两幅图直接自动生成图像插帧，从而在保证动画质量的基础上大大减少了动画制作者的成本和制作时间。虽然现在这些技术还没有普遍应用在动画中，但可以肯定随着机器学习模型进一步的完善和训练，深度学习生成动漫图像来替代人工手作图已是必然趋势，之后的画师不必再花大量的时间去设计人物，也不需要花大量的精力为了动漫的流畅度去一帧帧的画图，只需一些简单的线稿，一些动画镜头的关键帧，剩下的就只需要交给深度学习模型去完成这部动画。当绘画和动画的工作不再是主要制作成本和时间后，不仅仅会推动动漫领域的发展，电影，游戏以及更多需要可视化图像的领域都能被加速推动发展。 ​ 本文我们将采用StyleGAN深度学习模型进行训练，达到通过随机种子的输入，得到随机动漫人物头像图片的结果。在该模型已有参考代码的情况下，其关键点就在于配置好Python环境，制作庞大且合适的训练集以及调整初始参数。 二 项目目标1.项目目标​ 制作合适的训练集，利用StyleGAN深度学习模型训练，可以随机生成动漫人物头像图片。 2.基本原理与模型2.1 极大似然估计​ 在理解GAN生成图像原理前，我们先从极大似然估计说起。我们用$P(x,\\theta)$表示每个样本x分布的概率，其中$\\theta$表示该分布的参数。那么极大似然估计需要解决的就是已知一个数据分布$P_{data}(x)$，再给定一个有参数$\\theta$定义的数据分布$P_{G}(x,\\theta)$，我们希望求得参数$\\theta$使得$P_{G}(x,\\theta)$尽可能接近$P_{data}(x)$。那么在图像生成中，我们导入的训练集的图片，其每个像素点每个通道的灰度值的集合就相当于一个数据分布$P_{data}(x)$，我们要做的就是寻找一个$P_{G}(x,\\theta)$去接近这一分布，这样当我们再随机代入参数$\\theta$值时，就相当于生成了一个和数据集图片类似但不重复的新图片。 ​ 那么重点就在于怎样求得参数$\\theta$，理论上需要三步： ​ 1.从$P_{data}(x)$采样m个样本{$x^1,x^2,…,x^m$} ​ 2.计算采样样本的似然函数 $L=\\prod_{i=1}^m P_G(x^i;\\theta)$ ​ 3.计算使得似然函数$L$最大的参数$\\theta$：$\\theta^{*}={\\underset{\\theta}{\\operatorname{arg\\,max}}}\\ L={\\underset{\\theta}{\\operatorname{arg\\,max}}}\\prod_{i=1}^m P_G(x^i;\\theta)$ ​ 我们继续上式的计算： \\begin{aligned} \\theta^*&={\\underset{\\theta}{\\operatorname{arg\\,max}}}\\ L \\\\ &\\approx {\\underset{\\theta}{\\operatorname{arg\\,max}}}\\ E_{x\\backsim P_{data}}[\\operatorname{log}P_G(x;\\theta)]\\\\ &={\\underset{\\theta}{\\operatorname{arg\\,max}}}\\ \\int_x P_{data}(x)\\operatorname{log}P_G(x;\\theta)dx-\\int_x P_{data}(x)\\operatorname{log}P_{data}(x)dx\\\\ &={\\underset{\\theta}{\\operatorname{arg\\,min}}}\\ KL(P_{data}(x)||P_G(x;\\theta)) \\end{aligned}​ 其中用到了KL散度：KL(P||Q)，用于衡量P，Q这两个概率分布差异的方式： KL(P||Q)=\\int_x p(x)(\\operatorname{log}p(x)-\\operatorname{log}q(x))​ 即我们找到一个$\\theta$使得$P_G(x;\\theta)$与目标分布$P_{data}(x)$的KL散度尽可能低，即可确定参数$\\theta$。 2.2 GAN的基本原理​ 知道极大似然估计对于图像生成的帮助后，GAN本质上就是在做极大似然估计的事情，我们希望可以用某一种具体的分布形式$P_G(x;\\theta)$尽可能逼真地表达分布$P_{data}(x)$，这样我们就相当于得到了$P_{data}(x)$，并据此分布$P_G(x;\\theta)$采样，即做生成任务。由于神经网络具有强大拟合能力，我们设计一个神经网络G来得到更普适的$P_G(x;\\theta)$，大致的结构图如下： ​ 我们先选取一个简单的先验分布$P_{prior}$,并从该先验分布中采样$z$作为输入，输入到神经网络$G$，得$G(z)=x$生成图像$x$。我们通过这种方式构建了生成分布$P_{G}{x;\\theta}$。此时该分布主要由神经网络$G$决定，参数$\\theta$由网络参数定义，我们可以通过输入$z$来在该分布上采样$x$。类似极大似然估计，我们通过比较两个分布样本的差异设计loss来调节优化神经网络$G$的参数$\\theta$，从而实现将分布$P_G$向$P_{data}$拉近，达到用$P_G$拟合表达$P_{data}$的效果。不过，直接计算$\\theta$值是不可能的，这时候我们就需要用到GAN来帮助我们拟合。 ​ GAN由生成器G和判别器D组成，上一段其实就是生成器G所做的工作，即G就是一个函数，输入$z\\backsim P_{prior}$，输出$x\\backsim P_G$。而判别器D的作用就是用来评估$P_G(x,\\theta)$和$P_{data}(x)$之间的差异，本质上也是一个函数，输入$x\\backsim P_G$,输出一个数值来衡量差异。GAN最后的目标用符号化语言表示就是： G^*=\\operatorname{arg}\\ {\\underset{G}{\\operatorname{min}}}\\ {\\underset{D}{\\operatorname{max}}}\\ V(G,D)我们的目标是得到使得式子${\\underset{D}{\\operatorname{max}}}\\ V(G,D)$最小的生成器$G^*$。 ​ 关于$V$: V(G,D)=E_{x\\backsim P_{data}}[\\operatorname{log}D(x)]+E_{x\\backsim P_{G}}[1-\\operatorname{log}D(x)]​ 那么我们该如何优化获得$G^*$呢？这里可以通过梯度下降法来实现： \\theta_G \\rightarrow \\theta_G -\\eta \\frac{\\partial L(G)}{\\partial \\theta_G}其中$ L(G) = {\\underset{D}{\\operatorname{max}}}\\ V(G,D) = V^\\star(G,D^\\star) $ . 我们将D看为二分类器，那么确定D，就可以使用梯度下降来优化D的最终参数。我们可以将 $ {\\underset{D}{\\operatorname{max}}}\\ V(G,D) $从期望值计算改写为样本计算（近似估计）： \\tilde{V}=\\frac{1}{m}\\sum_{i=1}^{m}\\operatorname{log}D(x^i)+\\frac{1}{m}\\sum_{i=1}^{m}\\operatorname{log}(1-D(\\tilde{x^i}))则最终GAN的算法流程为： 1.初始化参数$\\theta_D$和$\\theta_G$ 2.学习优化判别器D： ​ $\\cdot$ 从$P_{data}(x)$采样{$x^1,x^2,…,x^m$} ​ $\\cdot$从$P_{prior}(z)$采样{$z^1,z^2,…,z^m$} ​ $\\cdot$通过生成器$\\tilde{x^i}=G(z^i)$获得生成样本{$\\tilde{x^1},\\tilde{x^2},…,\\tilde{x^m}$} ​ $\\cdot$梯度下降更新$\\theta_D$来最大化$\\tilde{V}=\\frac{1}{m}\\sum_{i=1}^{m}\\operatorname{log}D(x^i)+\\frac{1}{m}\\sum_{i=1}^{m}\\operatorname{log}(1-D(\\tilde{x^i}))$ 3.学习优化生成器G： ​ $\\cdot$再从$P_{prior}(z)$采样另一组{$z^1,z^2,…,z^m$} ​ $\\cdot$梯度下降更新$\\theta_G$来最小化$\\tilde{V}=\\frac{1}{m}\\sum_{i=1}^{m}\\operatorname{log}D(x^i)+\\frac{1}{m}\\sum_{i=1}^{m}\\operatorname{log}(1-D(G(z^i)))$ 2.3 StyleGAN对于人脸生成的优化Paper:[1812.04948] A Style-Based Generator Architecture for Generative Adversarial Networks (arxiv.org) ​ StyleGAN，提出了一个新的 generator architecture，号称能够控制所生成图像的高层级属性(high-level attributes)，如发型、雀斑等，并且生成的图像在一些评价标准上得分更好，这也是我们最终选择这个模型的原因。其相对于传统GAN有以下改进： （1）Style-based generator ​ 不同于常见的直接把 latent code 输入给 generator 的输入层的做法，该模型摒弃了输入层， 转而添加了一个非线性映射网络：$f:Z\\rightarrow W$，如下图： 图中，z和w的维度都是512维，A是一个仿射变换(Affine transform)，B是每个channel的高斯噪声的系数，而AdaIN则是： 图中，$y(s,i)$和$y(b,i)$则是w经过A变换之后得到的$y=(y_s,y_b),(y(s,i),y(b,i))$对的个数与每一层feature map的channel数相同。 （2）Style mixing ​ 为进一步促使这些style来局部化其控制效果，作者又采用了mixing regularization。style-based generator的8层卷积的非线性变换中，提前计算两个图像的变换结果$w_1、w_2$，然后在生成过程中随机取$w_1$或者$w_2$的值进行操作。如下图： （3）Stochastic variation ​ 人有很多随机属性，例如头发、胡须、雀斑等，所以一个好的generator自然也要能够实现stochastic variation。而该论文作者认为，传统的generator的输入就只有输入层，所以generator自身必须寻找一种生成伪随机数的方式，而这会消耗网络的capacity，并且很难隐藏生成的 信号的周期性。而这时，该模型的高斯噪声输入B就派上了用场了。如下图： 三 项目具体实现主要分为三步：爬取图片、截取人脸部分、部署环境运行代码。 1.爬虫1.1 Pixivhttps://www.pixiv.net是一个插画分享网站,不过大部分情况下直接搜索tag只能得到按照时间排序的结果,结果通常参差不齐,很难看到高质量的作品: 按照热度排序还需要付费会员,很不划算,所以我写了多种爬虫策略.最终项目包含四个程序:findPID.py,findFollow.py,getInfo.py,main.py. 包含的功能包括爬取指定id的插画,按照作者id爬取作品,爬取日榜,周榜,月榜,爬取公开关注画师的作品,爬取悄悄关注画师的作品等.当然这个关注的画师和程序中用到的cookies有关,有需要可以替换成自己的cookies. 1.1.1 fetInfo.py一一介绍一下包含的函数功能 GetLike(id) 输入作品id,返回这个作品的点赞数 GetBookmark(id) 输入作品id,返回这个作品的收藏数 GetView(id) 输入作品id,返回这个作品的浏览数 getHideFollow() 返回悄悄关注的画师id列表 getShowFollow() 返回公开关注的画师id列表 1.1.2 findPID.pyfindPIDs_by_Author(pid) 输入作者id,返回这个作者的全部插画id列表 findPIDs_by_Daily() 返回今日的日榜中的全部id列表 findPIDs_by_Weekly() 返回今日的周榜中的全部id列表 findPIDs_by_Monthly() 返回今日的月榜中的全部id列表 findPIDs_by_Male() 返回今日的男性日榜中的全部id列表 findPIDs_by_Female() 返回今日的女性日榜中的全部id列表 findPIDs_by_Tag() 需要输入搜索模式,tag数量以及tag,返回这个tag搜索下得到的全部作品的id列表(注:Pixiv搜索展示有上限6w张,所以时间过于久远的作品是搜不出来的) findPIDs_by_Tag_sort() 与上一个函数类似,区别是将结果进行了排序,可以按收藏数,点赞数,浏览数排序,相当于手动解锁了按热度排序的功能,不过耗时较久. 1.1.3 findFollow.pyfindFollow(mode) 输入mode,1表示公开关注的,2表示悄悄关注的,返回关注的画师id列表 1.1.4 main.py一般情况直接运行该程序即可,会有详细的引导. 1.2 Konachanhttp://konachan.net/也是一个插画分享网站,直接在代码40,41行修改爬取的起始页面与终止页面.直接运行即可. 随后将两个程序部署在了vultr服务器上,主要是可以直连pixiv,利用rclone挂载上本人OneDrive,将爬下来的图片保存在OneDrive中,最终可以得到将近四百G图片. 2.人脸提取参考了这个GitHub上的项目:https://github.com/nagadomi/lbpcascade_animeface 最终提取得到2w张人脸图片,共享至OneDrive:https://1drv.ms/u/s!AkFaXpu1Wty9k4NmlYU2rER15Au8QA?e=GxjwqY 3.部署环境运行代码分别在两台电脑上运行了两个GitHub的项目。 第一台电脑上的环境部署参考github源码：https://github.com/NVlabs/stylegan2.git 所用计算机：Windows10 ，显卡：NVDIA GeForce GTX 1060 Python版本：3.6.13 所用到的包：见所附 requirments1.txt 文件。 基本步骤：安装Git，获取GitHub上的项目源码，安装2017版本的VS2017，安装CUDA和cuDNN，安装Anaconda，创建虚拟环境，修改Windows的环境变量路径，在命令行终端运行。 1 安装git https://git-scm.com/downloads 2 获取源码 创建一个项目文件夹，并使用cd命令进入该文件，输入一下代码获取 1git clone https://github.com/NVlabs/stylegan2.git 3 安装VS2017 https://docs.microsoft.com/en-us/visualstudio/releasenotes/vs2017-relnotes 4 安装CUDA及cuDNN 由于深度学习最好用到GPU参与（可以提高计算速度），而CUDA（Compute Unified Device Architecture）可以实现CPU与GPU并用的协同处理，因此需要下载。 限于本文参考项目源码的限制，需要下载CUDA10.0版本，下载地址为：https://developer.nvidia.com/cuda-10.0-download-archive； Cudnn的下载地址为：https://developer.nvidia.com/rdp/cudnn-archive，选择cuDNNv7.5.0 for CUDA10.0 的windows10版本，将下载到的文件拷贝到CUDA的同名目录即可。 5 anaconda的安装 anaonda包含了conda、Python等180多个科学包及其依赖项，其中的conda可以实现在同一个机器上安装不同版本的软件包及其依赖，并能够在不同的环境之间切换，非常方便，由于项目源码的年代比较久远，因此需要用conda创建python虚拟环境。 下载地址：https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/，下载版本为：3-5.2.0-windows-x86_64.exe 6 创建虚拟环境 123conda create -n your_env_name python=x.x # 使用conda创建虚拟环境activate your_env_name #激活虚拟环境pip install -r requirements.txt #根据依赖文件批量安装库，txt文件已附 7 修改windows的环境变量路径 将conda、CUDA10、VS2017等加入到PATH中。 8 运行程序 1python run_generator.py generate-images --network=gdrive:networks/stylegan2-ffhq-config-f.pkl --seeds=6600-6625 --truncation-psi=0.5 在./results/文件夹下可以看到结果。 第二台电脑上的环境部署关键是Python、PyTorch、CUDA、visual studio的版本要匹配,之前失败很多次都是版本不匹配的原因。 在显卡为RTX3070的电脑上部署StyleGAN项目时频繁遇到问题,发现GitHub上的很多项目都是用的TensorFlow1.x,这些版本最高只支持到CUDA10,但是RTX3070只支持CUDA11以上的版本,TensorFlow1和2的版本问题很让人头疼,于是考虑转而使用PyTorch。 参考github源码：https://github.com/johnhany/stylegan2-pytorch 所用计算机：Windows10 ，显卡：NVDIA GeForce RTX 3070 Python版本：3.7.10 所用到的包：见所附 requirments2.txt 文件。 基本步骤：获取GitHub上的项目源码，安装2019版本的VS2019，安装CUDA11.1和cuDNN8.0.4，创建虚拟环境，修改Windows的环境变量路径，pip安装所需要的包，将图片放在指定路径./images/,运行: 1stylegan2_pytorch --data ./images/ --num_workers=0 随后在./results/文件夹下可以看到结果。 关于运行结果分别见result1 和result2文件夹。 四 结果展示最终生成的图像选取一部分如下，有些已经可以达到以假乱真的效果了 五 关于GAN的进一步讨论GAN优点GAN的核心思想是基于通过鉴别器进行的“间接”训练，而这种训练本身也是动态更新的这基本上意味着生成器没有被训练去最小化到特定图像的距离，而是欺骗鉴别器，这使得模型能够以无监督的方式学习，GAN是隐式生成模型，这意味着它们不显式地建模似然函数。虽然GAN最初是作为一种无监督学习的生成模型提出的，但事实证明，GAN也适用于半监督学习、完全监督学习、和强化学习。能更好建模数据分布（图像更锐利、清晰）。理论上，GANs 能训练任何一种生成器网络。其他的框架需要生成器网络有一些特定的函数形式，比如输出层是高斯的。 GAN缺点无需利用马尔科夫链反复采样，无需在学习过程中进行推断，没有复杂的变分下界，避开近似计算棘手的概率的难题。难训练，不稳定。生成器和判别器之间需要很好的同步，但是在实际训练中很容易D收敛，G发散。D/G 的训练需要精心的设计。模式缺失（Mode Collapse）问题。GANs的学习过程可能出现模式缺失，生成器开始退化，总是生成同样的样本点，无法继续学习。 GAN应用 GAN可以用来生成艺术图像，GAN还可以用来修复照片，或者为想象中的时装模特创作照片； GAN可以用来生成天文学图像，模拟暗物质研究中的引力透镜效应； GAN是一种通过高能物理实验的量热计快速准确地模拟高能射流形成和簇射的方法； 在电子游戏领域，GAN根据低分辨率的2D图像训练生成高分辨率的4k图像，然后根据原始图像1的分辨率进行采样，可以使原始图像更加清晰，获得更加清晰的纹理，同时不丢失原始图像的色彩、细节。 在迁移学习方面，最先进的迁移学习研究使用GAN来加强潜在特征空间的对齐，例如在深度强化学习中。 其它应用，gan可以从图像、和视频中的运动模型模式中重建物体的三维模型，可用于显示人的外表随着年龄的变化；一种GAN模型——Speech2Face可以通过人的声音重建其外貌；GAN也可处理时间序列的数据，例如，recurrent GANs 可以生成能量数据用于机器学习。 GAN引起的担忧（危害）可能会将GAN用于可能会导致犯罪的照片和视频，创建虚假的个人资料，使用图像合成技术生成涩情视频等，具有极大的潜在危害。 六 关于本项目的总结本文使用style生成基于从Pixiv网站爬取的二次元图像，然后截取图像中的脸部图像，经过Style-GAN生成新的二次元图像。与根据大量真实人脸的照片生成并不存在的人脸照片类似，通过多次训练，完全可以达到以假乱真的效果，只不过由于设备原因（gpu性能不高只使用了单个gpu），代码运行时间过长，可以预计未来数据量提升，必然会带动计算能力的提升，二者形成正循环，同时也意味着人类的隐私会越来越少，人脸、声音等信息都会被作为数据进行存储。","link":"/2021/08/28/Image-Generator-Application-and-Implementation-of-StyleGAN/"},{"title":"Image Generator: Application and Implementation of StyleGAN","text":"Image Generator: Application and Implementation of StyleGAN I. Project Introduction II. Project Goals 1. Project Goals 2. Basic Principles and Models 2.1 Maximum Likelihood Estimation 2.2 Basic Principle of GAN 2.3 StyleGAN’s Optimization for Face Generation III. Project Implementation 1. Web Scraping 1.1 Pixiv 1.1.1 getInfo.py 1.1.2 findPID.py 1.1.3 findFollow.py 1.1.4 main.py 1.2 Konachan 2. Face Extraction 3. Deploying the Environment and Running the Code Environment Deployment on the First Computer Environment Deployment on the Second Computer IV: Result Presentation V: Further Discussion on GAN Advantages of GAN Disadvantages of GAN Applications of GAN Concerns Raised by GAN (Harm) VI: Summary of This Project Image Generator: Application and Implementation of StyleGAN*note: This blog is an English version of previous blog. I. Project Introduction Deep learning is one of the most significant breakthroughs in the field of artificial intelligence in the past decade. It has achieved immense success in many fields such as speech recognition, natural language processing, computer vision, image and video analysis, multimedia, and more. Among these, the Generative Adversarial Networks (GAN) model is one of the most promising methods for unsupervised learning on complex distributions in recent years. It is widely used in image generation and recognition areas, such as neural style transfer, Google’s developed Deep Dream algorithm, and variational autoencoders. Recently in the field of anime drawing, attempts have been made to use machine learning to create 2D anime images, thereby replacing traditional hand-drawing and 3-to-2 conversions, which are more labor-intensive and time-consuming. Cinnamon AI has begun to offer automatic coloring features for anime, capable of completing certain pre-processing tasks like filling in missing lines in sketches and automatic coloring, reducing the time required for coloring single images by 1/10 and cutting costs by over 50%. This lessens the workload for artists and enhances productivity for anime creators. ATD-12K is a large-scale dataset for animation triplets that can automatically generate image in-betweening between two images, significantly reducing costs and production time for animators while ensuring animation quality. Although these technologies have not been widely applied in animation yet, it is certain that as machine learning models continue to improve and train, deep learning-generated anime images replacing manual drawings is an inevitable trend. Future artists will not need to spend a significant amount of time designing characters and painstakingly drawing each frame for smooth animation. They will only need some simple sketches and some key frames for animation shots. The rest can be left to deep learning models to complete the animation. When the work of drawing and animation is no longer a major production cost and time factor, it will not only drive the development of the anime industry but also accelerate the growth in movies, games, and many more fields requiring visual images. In this article, we will use the StyleGAN deep learning model for training, achieving results of random anime character portraits by inputting random seeds. With existing reference codes for this model, the key points lie in setting up the Python environment, creating a large and suitable training set, and adjusting the initial parameters. II. Project Goals1. Project GoalsThe purpose of this project is to generate a suitable training set, use the StyleGAN deep learning model for training, and randomly generate anime character avatar images. 2. Basic Principles and Models2.1 Maximum Likelihood EstimationBefore understanding the principle of GAN image generation, let’s start with maximum likelihood estimation. We use $P(x,\\theta)$ to represent the probability distribution of each sample x, where $\\theta$ represents the parameters of this distribution. The problem that maximum likelihood estimation needs to solve is that given a data distribution $P_{data}(x)$ and a data distribution $P_{G}(x,\\theta)$ defined by parameters $\\theta$, we hope to find the parameter $\\theta$ that makes $P_{G}(x,\\theta)$ as close as possible to $P_{data}(x)$. Therefore, in image generation, the collection of grayscale values of each pixel and each channel of the pictures in our imported training set is equivalent to a data distribution $P_{data}(x)$. What we have to do is to find a $P_{G}(x,\\theta)$ to approximate this distribution, so that when we randomly substitute the parameter $\\theta$ value, it is equivalent to generating a new picture similar to but not repeated from the dataset. The key is how to obtain the parameter $\\theta$, theoretically it requires three steps: Sample m samples {$x^1,x^2,…,x^m$} from $P_{data}(x)$ Calculate the likelihood function of the sampled sample $L=\\prod_{i=1}^m P_G(x^i;\\theta)$ Calculate the parameter $\\theta$ that makes the likelihood function $L$ the largest: $\\theta^{*}={\\underset{\\theta}{\\operatorname{arg\\,max}}}\\ L={\\underset{\\theta}{\\operatorname{arg\\,max}}}\\prod_{i=1}^m P_G(x^i;\\theta)$ We continue the calculation of the above formula:$\\begin{aligned}\\theta^*&amp;={\\underset{\\theta}{\\operatorname{arg\\,max}}}\\ L \\\\&amp;\\approx {\\underset{\\theta}{\\operatorname{arg\\,max}}}\\ E_{x\\backsim P_{data}}[\\operatorname{log}P_G(x;\\theta)]\\\\&amp;={\\underset{\\theta}{\\operatorname{arg\\,max}}}\\ \\int_x P_{data}(x)\\operatorname{log}P_G(x;\\theta)dx-\\int_x P_{data}(x)\\operatorname{log}P_{data}(x)dx\\\\&amp;={\\underset{\\theta}{\\operatorname{arg\\,min}}}\\ KL(P_{data}(x)||P_G(x;\\theta))\\end{aligned}$ Here we use the KL divergence: KL(P||Q), a method to measure the difference between the two probability distributions P and Q:$KL(P||Q)=\\int_x p(x)(\\operatorname{log}p(x)-\\operatorname{log}q(x))$ That is, we find a $\\theta$ so that the KL divergence of $P_G(x;\\theta)$ and the target distribution $P_{data}(x)$ is as low as possible, which can determine the parameter $\\theta$. 2.2 Basic Principle of GANKnowing the help of maximum likelihood estimation for image generation, GAN is essentially doing the work of maximum likelihood estimation. We hope that we can use a specific distribution form $P_G(x;\\theta)$ to express the distribution $P_{data}(x)$ as realistically as possible, so that we have obtained $P_{data}(x)$, and sample according to this distribution $P_G(x;\\theta)$, that is, perform generation tasks. Since neural networks have strong fitting capabilities, we design a neural network G to obtain a more general $P_G(x;\\theta)$. The general structure diagram is as follows: We first select a simple prior distribution $P_{prior}$, and sample $z$ from this prior distribution as input, input to the neural network $G$, and get $G(z)=x$ to generate the image $x$. In this way, we have constructed the generation distribution $P_{G}{x;\\theta}$. At this time, this distribution is mainly determined by the neural network $G$, the parameter $\\theta$ is defined by the network parameters, and we can sample $x$ on this distribution by inputting $z$. Similar to maximum likelihood estimation, we design loss to adjust and optimize the parameters $\\theta$ of the neural network $G$ by comparing the differences between the two distribution samples, thereby realizing the effect of fitting and expressing $P_{data}$ with $P_G$. However, it is impossible to directly calculate the $\\theta$ value, and we need to use GAN to help us fit at this time. GAN consists of a generator G and a discriminator D. The previous paragraph is actually the work done by the generator G, that is, G is a function, input $z\\backsim P_{prior}$, output $x\\backsim P_G$. The role of the discriminator D is to evaluate the difference between $P_G(x,\\theta)$ and $P_{data}(x)$, essentially also a function, input $x\\backsim P_G$, output a value to measure the difference. The final goal of GAN is expressed in symbolic language as: $ G^*=\\operatorname{arg}\\ {\\underset{G}{\\operatorname{min}}}\\ {\\underset{D}{\\operatorname{max}}}\\ V(G,D) $ Our goal is to get the generator $G^*$ that minimizes the formula ${\\underset{D}{\\operatorname{max}}}\\ V(G,D)$. About $V$: $ V(G,D)=E_{x\\backsim P_{data}}[\\operatorname{log}D(x)]+E_{x\\backsim P_{G}}[1-\\operatorname{log}D(x)] $ So how can we optimize to get $G^*$? Here it can be achieved by gradient descent: $ \\theta_G \\rightarrow \\theta_G -\\eta \\frac{\\partial L(G)}{\\partial \\theta_G} $ Where $ L(G) = {\\underset{D}{\\operatorname{max}}}\\ V(G,D) = V^\\star(G,D^\\star) $ . We regard D as a binary classifier, so if D is determined, we can use gradient descent to optimize the final parameters of D. We can rewrite $ {\\underset{D}{\\operatorname{max}}}\\ V(G,D) $ from expected value calculation to sample calculation (approximate estimation): $ \\tilde{V}=\\frac{1}{m}\\sum_{i=1}^{m}\\operatorname{log}D(x^i)+\\frac{1}{m}\\sum_{i=1}^{m}\\operatorname{log}(1-D(\\tilde{x^i})) $ So the final algorithm flow of GAN is: 1. Initialize parameters $\\theta_D$ and $\\theta_G$ 2. Learn to optimize the discriminator D: $\\cdot$ Sample {$x^1,x^2,...,x^m$} from $P_{data}(x)$ $\\cdot$Sample {$z^1,z^2,...,z^m$} from $P_{prior}(z)$ $\\cdot$Get the generated sample {$\\tilde{x^1},\\tilde{x^2},...,\\tilde{x^m}$} through the generator $\\tilde{x^i}=G(z^i)$ $\\cdot$Use gradient descent to update $\\theta_D$ to maximize $\\tilde{V}=\\frac{1}{m}\\sum_{i=1}^{m}\\operatorname{log}D(x^i)+\\frac{1}{m}\\sum_{i=1}^{m}\\operatorname{log}(1-D(\\tilde{x^i}))$ 3. Learn to optimize the generator G: $\\cdot$Sample another group {$z^1,z^2,...,z^m$} from $P_{prior}(z)$ again $\\cdot$Use gradient descent to update $\\theta_G$ to minimize $\\tilde{V}=\\frac{1}{m}\\sum_{i=1}^{m}\\operatorname{log}D(x^i)+\\frac{1}{m}\\sum_{i=1}^{m}\\operatorname{log}(1-D(G(z^i)))$ Let's move to the next part. #### 2.3 StyleGAN's Optimization for Face Generation Paper: [[1812.04948\\] A Style-Based Generator Architecture for Generative Adversarial Networks (arxiv.org)](https://arxiv.org/abs/1812.04948) StyleGAN introduces a new generator architecture that claims to control high-level attributes of the generated images, such as hairstyles, freckles, etc., and the generated images score better on some evaluation standards, which is why we finally chose this model. It has the following improvements compared to traditional GAN: (1) Style-based generator Unlike the common practice of directly inputting the latent code into the input layer of the generator, this model discards the input layer and instead adds a nonlinear mapping network: $f:Z\\rightarrow W$, as shown below: In the figure, both z and w are 512-dimensional, A is an affine transform, B is the coefficient of Gaussian noise for each channel, and AdaIN is: In the figure, $y(s,i)$ and $y(b,i)$ are $y=(y_s,y_b),(y(s,i),y(b,i))$ pairs obtained after w is transformed by A. The number of pairs is the same as the number of channels in each layer of the feature map. (2) Style mixing To further encourage these styles to localize their control effects, the author also adopted mixing regularization. In the eight-layer convolutional nonlinear transformation of the style-based generator, the transformation results of two images, $w_1$ and $w_2$, are calculated in advance, and then the values of $w_1$ or $w_2$ are randomly taken for operation during generation. As shown below: (3) Stochastic variation Humans have many random attributes, such as hair, beard, freckles, etc., so a good generator naturally needs to achieve stochastic variation. The authors of this paper believe that the input of traditional generators is only the input layer, so the generator itself must find a way to generate pseudorandom numbers, which will consume the network’s capacity, and it is difficult to hide the periodicity of the generated signals. At this time, the Gaussian noise input B of this model comes in handy. As shown below: III. Project ImplementationThe project is mainly divided into three steps: crawling images, cropping the face part, and deploying the environment to run the code. 1. Web Scraping1.1 PixivPixiv is an illustration sharing website. However, in most cases, directly searching for tags can only yield results sorted by time, which are usually uneven, and it’s hard to see high-quality works: Sorting by popularity requires a paid membership, which is not cost-effective, so I wrote multiple crawling strategies. The final project includes four programs: findPID.py, findFollow.py, getInfo.py, main.py. The included functions include crawling illustrations with a specified id, crawling works by author id, crawling daily, weekly, and monthly rankings, crawling works of publicly followed artists, crawling works of quietly followed artists, etc. Of course, the followed artists and cookies used in the program are related. If needed, you can replace them with your own cookies. 1.1.1 getInfo.pyLet me introduce the function of each function one by one GetLike(id) Input the work id and return the number of likes for this work GetBookmark(id) Input the work id and return the number of bookmarks for this work GetView(id) Input the work id and return the number of views for this work getHideFollow() Returns a list of quietly followed artist ids getShowFollow() Returns a list of publicly followed artist ids 1.1.2 findPID.pyfindPIDs_by_Author(pid) Enter the author id and return the list of all illustration ids of this author findPIDs_by_Daily() Returns the list of all ids in today’s daily ranking findPIDs_by_Weekly() Returns the list of all ids in today’s weekly ranking findPIDs_by_Monthly() Returns the list of all ids in today’s monthly ranking findPIDs_by_Male() Returns the list of all ids in today’s male daily ranking findPIDs_by_Female() Returns the list of all ids in today’s female daily ranking findPIDs_by_Tag() You need to input the search mode, tag number, and tag, and return the list of all work ids under this tag search (Note: Pixiv search display has a limit of 60k images, so works from too long ago cannot be searched) findPIDs_by_Tag_sort() Similar to the previous function, the difference is that the results are sorted, which can be sorted by the number of bookmarks, likes, views, equivalent to manually unlocking the function of sorting by popularity, but it takes longer. 1.1.3 findFollow.pyfindFollow(mode) Enter mode, 1 represents publicly followed, 2 represents quietly followed, return the list of followed artist ids 1.1.4 main.pyIn general, just run this program, there will be detailed guidance. 1.2 KonachanKonachan is also an illustration sharing website. Directly modify the start and end pages to be crawled in lines 40 and 41 of the code. Just run it directly. Then the two programs are deployed on the vultr server, mainly because it can directly connect to pixiv, use rclone to mount my OneDrive, and save the crawled pictures in OneDrive. In the end, you can get nearly 400G of pictures. 2. Face ExtractionThis step references the project on GitHub: https://github.com/nagadomi/lbpcascade_animeface Finally, 20,000 face images were extracted and shared on OneDrive: https://1drv.ms/u/s!AkFaXpu1Wty9k4NmlYU2rER15Au8QA?e=GxjwqY 3. Deploying the Environment and Running the CodeTwo GitHub projects were run on two computers respectively. Environment Deployment on the First ComputerReferencing the GitHub source code: https://github.com/NVlabs/stylegan2.git Computer used: Windows 10, Graphics card: NVDIA GeForce GTX 1060 Python version: 3.6.13 Packages used: See the attached requirments1.txt file. Basic steps: Install Git, get the project source code from GitHub, install VS2017 (2017 version), install CUDA and cuDNN, install Anaconda, create a virtual environment, modify the path of Windows environment variables, run from the command line terminal. 1 Install git https://git-scm.com/downloads 2 Get the source code Create a project folder, use the cd command to enter the folder, and input the following code to get it 1git clone https://github.com/NVlabs/stylegan2.git 3 Install VS2017 https://docs.microsoft.com/en-us/visualstudio/releasenotes/vs2017-relnotes 4 Install CUDA and cuDNN Because deep learning is best to use GPU participation (which can improve the computation speed), and CUDA (Compute Unified Device Architecture) can achieve the cooperative processing of CPU and GPU, it needs to be downloaded. Due to the limitations of the source code of the project referenced in this article, CUDA10.0 version needs to be downloaded. The download address is: https://developer.nvidia.com/cuda-10.0-download-archive; The download address for Cudnn is: https://developer.nvidia.com/rdp/cudnn-archive. Choose cuDNNv7.5.0 for CUDA10.0 Windows10 version, and copy the downloaded files to the same name directory of CUDA. 5 Anaconda installation Anaconda includes more than 180 scientific packages such as conda, Python, and their dependencies. The conda in it can install different versions of software packages and their dependencies on the same machine, and can switch between different environments, which is very convenient. Because the project source code is relatively old, it is necessary to use conda to create a python virtual environment. Download address: https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/, download version: 3-5.2.0-windows-x86_64.exe 6 Create a virtual environment 123conda create -n your_env_name python=x.x # Use conda to create a virtual environmentactivate your_env_name #Activate the virtual environmentpip install -r requirements.txt #Install libraries in batches according to the dependency file, txt file is attached 7 Modify the path of Windows environment variables Add conda, CUDA10, VS2017, etc. to PATH. 8 Run the program 1python run_generator.py generate-images --network=gdrive:networks/stylegan2-ffhq-config-f.pkl --seeds=6600-6625 --truncation-psi=0.5 You can see the results in the ./results/ folder. Environment Deployment on the Second ComputerThe key is that the versions of Python, PyTorch, CUDA, and Visual Studio need to match. Many previous failures were due to version mismatches. When deploying the StyleGAN project on a computer with an RTX3070 graphics card, frequent problems were encountered. It was found that many projects on GitHub use TensorFlow1.x, these versions only support up to CUDA10, but RTX3070 only supports CUDA11 and above. The version issues between TensorFlow1 and 2 are quite troublesome, so it was decided to switch to using PyTorch. Referencing the GitHub source code: https://github.com/johnhany/stylegan2-pytorch Computer used: Windows 10, Graphics card: NVDIA GeForce RTX 3070 Python version: 3.7.10 Packages used: See the attached requirments2.txt file. Basic steps: Get the project source code from GitHub, install VS2019 (2019 version), install CUDA11.1 and cuDNN8.0.4, create a virtual environment, modify the path of Windows environment variables, pip install the needed packages, put the images in the specified path ./images/, run: 1stylegan2_pytorch --data ./images/ --num_workers=0 Afterwards, you can see the results in the ./results/ folder. See the result1 and result2 folders for the respective running results. IV: Result PresentationA selection of the final generated images is shown below, some of which can achieve a very realistic effect. V: Further Discussion on GANAdvantages of GANThe core idea of GAN is based on “indirect” training through a discriminator, which is also dynamically updated. Basically, this means that the generator is not trained to minimize the distance to a specific image, but to deceive the discriminator, allowing the model to learn in an unsupervised manner. GAN is an implicit generative model, which means they do not explicitly model the likelihood function.Although GAN was initially proposed as an unsupervised learning generative model, it has been proven that GAN is also applicable to semi-supervised learning, fully supervised learning, and reinforcement learning.It can better model data distribution (sharper and clearer images).In theory, GANs can train any type of generator network. Other frameworks require the generator network to have some specific function forms, such as Gaussian output layers. Disadvantages of GANNo need to use Markov chains for repeated sampling, no inference during the learning process, no complex variational lower bounds, avoiding the tricky problem of approximating probabilities.Difficult to train, unstable. Good synchronization is needed between the generator and discriminator, but in actual training, it is easy for D to converge and G to diverge. The training of D/G needs careful design.Mode collapse problem. The learning process of GANs may experience mode collapse, where the generator begins to degenerate, always generating the same sample points, and cannot continue to learn. Applications of GAN GAN can be used to generate artistic images, GAN can also be used to repair photos, or create photos for imaginary fashion models; GAN can be used to generate astronomical images, simulating the gravitational lensing effect in dark matter research; GAN is a method to quickly and accurately simulate the formation of high-energy jets and cluster emission through calorimeters in high-energy physics experiments; In the field of video games, GAN trains high-resolution 4k images based on low-resolution 2D images, and then samples according to the resolution of the original image 1, which can make the original image clearer, obtain clearer textures, and at the same time not lose the color and details of the original image. In the field of transfer learning, the most advanced transfer learning research uses GAN to enhance the alignment of the latent feature space, such as in deep reinforcement learning. Other applications, GAN can reconstruct the three-dimensional model of objects from the motion model patterns in images and videos, can be used to display the appearance of people with age changes; a GAN model - Speech2Face can rebuild its appearance through people’s voices; GAN can also process time-series data, for example, recurrent GANs can generate energy data for machine learning. Concerns Raised by GAN (Harm)There are potential risks that GAN may be used for photos and videos that may lead to crimes, create fake profiles, generate explicit videos using image synthesis technology, etc. VI: Summary of This ProjectThis article uses StyleGAN to generate images based on the anime images crawled from the Pixiv website, then extracts the face images from the images, and generates new anime images through Style-GAN. Similar to generating non-existent face photos based on a large number of real face photos, through multiple training, it can completely achieve a very realistic effect. However, due to equipment reasons (the GPU performance is not high and only a single GPU is used), the code running time is too long. It can be expected that in the future, as the amount of data increases, it will inevitably drive the improvement of computing power, forming a positive cycle. At the same time, it also means that human privacy will be less and less, and information such as faces and voices will be stored as data.","link":"/2023/09/27/Image-Generator-Application-and-Implementation-of-StyleGAN-1/"}],"tags":[{"name":"Selenium","slug":"Selenium","link":"/tags/Selenium/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"PyTorch","slug":"PyTorch","link":"/tags/PyTorch/"},{"name":"TensorFlow","slug":"TensorFlow","link":"/tags/TensorFlow/"},{"name":"Web Crawler","slug":"Web-Crawler","link":"/tags/Web-Crawler/"},{"name":"Monte Carlo Method","slug":"Monte-Carlo-Method","link":"/tags/Monte-Carlo-Method/"},{"name":"Thermoelectricity","slug":"Thermoelectricity","link":"/tags/Thermoelectricity/"},{"name":"Magnetics","slug":"Magnetics","link":"/tags/Magnetics/"}],"categories":[{"name":"Project Summary","slug":"Project-Summary","link":"/categories/Project-Summary/"},{"name":"Learning Notes","slug":"Learning-Notes","link":"/categories/Learning-Notes/"},{"name":"Demo","slug":"Demo","link":"/categories/Demo/"}]}