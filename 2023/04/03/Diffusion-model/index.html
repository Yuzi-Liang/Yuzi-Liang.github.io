<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Diffusion model - Yuzi Liang | University of Science and Technology of China</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Yuzi Liang | University of Science and Technology of China"><meta name="msapplication-TileImage" content="/img/favicon2.ico"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Yuzi Liang | University of Science and Technology of China"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="A Diffusion Model from Scratch in Pytorch Investigating the dataset   Building the Diffusion Model Step 1: The forward process &amp;#x3D; Noise scheduler Step 2: The backward process &amp;#x3D; U-Net Step 3: The loss"><meta property="og:type" content="blog"><meta property="og:title" content="Diffusion model"><meta property="og:url" content="http://example.com/2023/04/03/Diffusion-model/"><meta property="og:site_name" content="Yuzi Liang | University of Science and Technology of China"><meta property="og:description" content="A Diffusion Model from Scratch in Pytorch Investigating the dataset   Building the Diffusion Model Step 1: The forward process &amp;#x3D; Noise scheduler Step 2: The backward process &amp;#x3D; U-Net Step 3: The loss"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:published_time" content="2023-04-04T04:22:22.000Z"><meta property="article:modified_time" content="2023-04-09T05:03:36.801Z"><meta property="article:author" content="Yuzi Liang"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="PyTorch"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2023/04/03/Diffusion-model/"},"headline":"Yuzi Liang | University of Science and Technology of China","image":["http://example.com/img/og_image.png"],"datePublished":"2023-04-04T04:22:22.000Z","dateModified":"2023-04-09T05:03:36.801Z","author":{"@type":"Person","name":"Yuzi Liang"},"description":"A Diffusion Model from Scratch in Pytorch Investigating the dataset   Building the Diffusion Model Step 1: The forward process &#x3D; Noise scheduler Step 2: The backward process &#x3D; U-Net Step 3: The loss"}</script><link rel="canonical" href="http://example.com/2023/04/03/Diffusion-model/"><link rel="icon" href="/img/favicon2.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/ustc_logo.jpg" alt="Yuzi Liang | University of Science and Technology of China" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-04-04T04:22:22.000Z" title="4/3/2023, 9:22:22 PM">2023-04-03</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-04-09T05:03:36.801Z" title="4/8/2023, 10:03:36 PM">2023-04-08</time></span><span class="level-item"><a class="link-muted" href="/categories/Learning-Notes/">Learning Notes</a></span><span class="level-item">12 minutes read (About 1806 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Diffusion model</h1><div class="content"><!-- toc -->
<ul>
<li><a href="#a-diffusion-model-from-scratch-in-pytorch">A Diffusion Model from Scratch in Pytorch</a><ul>
<li><a href="#investigating-the-dataset">Investigating the dataset</a></li>
</ul>
</li>
<li><a href="#building-the-diffusion-model">Building the Diffusion Model</a><ul>
<li><a href="#step-1-the-forward-process-noise-scheduler">Step 1: The forward process = Noise scheduler</a></li>
<li><a href="#step-2-the-backward-process-u-net">Step 2: The backward process = U-Net</a></li>
<li><a href="#step-3-the-loss">Step 3: The loss</a></li>
<li><a href="#sampling">Sampling</a></li>
<li><a href="#training">Training</a></li>
</ul>
</li>
<li><a href="#ways-to-improve">Ways to Improve</a></li>
</ul>
<!-- tocstop -->
<h1><span id="a-diffusion-model-from-scratch-in-pytorch">A Diffusion Model from Scratch in Pytorch</span></h1><p>Reposted from <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=a4Yfz2FxXiY&amp;t=884s">https://www.youtube.com/watch?v=a4Yfz2FxXiY&amp;t=884s</a></p>
<p>In this notebook I want to build a very simple (as few code as possible) Diffusion Model for generating car images. I will explain all the theoretical details in the YouTube video. </p>
<p><strong>Sources:</strong></p>
<ul>
<li>Github implementation <a target="_blank" rel="noopener" href="https://github.com/lucidrains/denoising-diffusion-pytorch">Denoising Diffusion Pytorch</a></li>
<li>Niels Rogge, Kashif Rasul, <a target="_blank" rel="noopener" href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb#scrollTo=3a159023">Huggingface notebook</a></li>
<li>Papers on Diffusion models ([Dhariwal, Nichol, 2021], [Ho et al., 2020] ect.)</li>
</ul>
<h2><span id="investigating-the-dataset">Investigating the dataset</span></h2><p>As dataset we use the StandordCars Dataset, which consists of around 8000 images in the train set. Letâ€™s see if this is enough to get good results ;-)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_images</span>(<span class="params">dataset, num_samples=<span class="number">20</span>, cols=<span class="number">4</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Plots some samples from the dataset &quot;&quot;&quot;</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">15</span>,<span class="number">15</span>)) </span><br><span class="line">    <span class="keyword">for</span> i, img <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataset):</span><br><span class="line">        <span class="keyword">if</span> i == num_samples:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        plt.subplot(math.ceil(num_samples/cols + <span class="number">1</span>), cols, i + <span class="number">1</span>)</span><br><span class="line">        plt.imshow(img[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">data = torchvision.datasets.StanfordCars(root=<span class="string">&quot;.&quot;</span>, download=<span class="literal">True</span>)</span><br><span class="line">show_images(data)</span><br></pre></td></tr></table></figure>
<p>Later in this notebook we will do some additional modifications to this dataset, for example make the images smaller, convert them to tensors ect. </p>
<h1><span id="building-the-diffusion-model">Building the Diffusion Model</span></h1><h2><span id="step-1-the-forward-process-noise-scheduler">Step 1: The forward process = Noise scheduler</span></h2><p>We first need to build the inputs for our model, which are more and more noisy images. Instead of doing this sequentially, we can use the closed form provided in the papers to calculate the image for any of the timesteps individually. </p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>The noise-levels/variances can be pre-computed</li>
<li>There are different types of variance schedules</li>
<li>We can sample each timestep image independently (Sums of Gaussians is also Gaussian)</li>
<li>No model is needed in this forward step</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_beta_schedule</span>(<span class="params">timesteps, start=<span class="number">0.0001</span>, end=<span class="number">0.02</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> torch.linspace(start, end, timesteps)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_index_from_list</span>(<span class="params">vals, t, x_shape</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">    Returns a specific index t of a passed list of values vals</span></span><br><span class="line"><span class="string">    while considering the batch dimension.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    batch_size = t.shape[<span class="number">0</span>]</span><br><span class="line">    out = vals.gather(-<span class="number">1</span>, t.cpu())</span><br><span class="line">    <span class="keyword">return</span> out.reshape(batch_size, *((<span class="number">1</span>,) * (<span class="built_in">len</span>(x_shape) - <span class="number">1</span>))).to(t.device)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_diffusion_sample</span>(<span class="params">x_0, t, device=<span class="string">&quot;cpu&quot;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">    Takes an image and a timestep as input and </span></span><br><span class="line"><span class="string">    returns the noisy version of it</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    noise = torch.randn_like(x_0)</span><br><span class="line">    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)</span><br><span class="line">    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(</span><br><span class="line">        sqrt_one_minus_alphas_cumprod, t, x_0.shape</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># mean + variance</span></span><br><span class="line">    <span class="keyword">return</span> sqrt_alphas_cumprod_t.to(device) * x_0.to(device) \</span><br><span class="line">    + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define beta schedule</span></span><br><span class="line">T = <span class="number">300</span></span><br><span class="line">betas = linear_beta_schedule(timesteps=T)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pre-calculate different terms for closed form</span></span><br><span class="line">alphas = <span class="number">1.</span> - betas</span><br><span class="line">alphas_cumprod = torch.cumprod(alphas, axis=<span class="number">0</span>)</span><br><span class="line">alphas_cumprod_prev = F.pad(alphas_cumprod[:-<span class="number">1</span>], (<span class="number">1</span>, <span class="number">0</span>), value=<span class="number">1.0</span>)</span><br><span class="line">sqrt_recip_alphas = torch.sqrt(<span class="number">1.0</span> / alphas)</span><br><span class="line">sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)</span><br><span class="line">sqrt_one_minus_alphas_cumprod = torch.sqrt(<span class="number">1.</span> - alphas_cumprod)</span><br><span class="line">posterior_variance = betas * (<span class="number">1.</span> - alphas_cumprod_prev) / (<span class="number">1.</span> - alphas_cumprod)</span><br></pre></td></tr></table></figure>
<p>Letâ€™s test it on our dataset â€¦</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">IMG_SIZE = <span class="number">64</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_transformed_dataset</span>():</span></span><br><span class="line">    data_transforms = [</span><br><span class="line">        transforms.Resize((IMG_SIZE, IMG_SIZE)),</span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(), <span class="comment"># Scales data into [0,1] </span></span><br><span class="line">        transforms.Lambda(<span class="keyword">lambda</span> t: (t * <span class="number">2</span>) - <span class="number">1</span>) <span class="comment"># Scale between [-1, 1] </span></span><br><span class="line">    ]</span><br><span class="line">    data_transform = transforms.Compose(data_transforms)</span><br><span class="line"></span><br><span class="line">    train = torchvision.datasets.StanfordCars(root=<span class="string">&quot;.&quot;</span>, download=<span class="literal">True</span>, </span><br><span class="line">                                         transform=data_transform)</span><br><span class="line"></span><br><span class="line">    test = torchvision.datasets.StanfordCars(root=<span class="string">&quot;.&quot;</span>, download=<span class="literal">True</span>, </span><br><span class="line">                                         transform=data_transform, split=<span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.utils.data.ConcatDataset([train, test])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tensor_image</span>(<span class="params">image</span>):</span></span><br><span class="line">    reverse_transforms = transforms.Compose([</span><br><span class="line">        transforms.Lambda(<span class="keyword">lambda</span> t: (t + <span class="number">1</span>) / <span class="number">2</span>),</span><br><span class="line">        transforms.Lambda(<span class="keyword">lambda</span> t: t.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)), <span class="comment"># CHW to HWC</span></span><br><span class="line">        transforms.Lambda(<span class="keyword">lambda</span> t: t * <span class="number">255.</span>),</span><br><span class="line">        transforms.Lambda(<span class="keyword">lambda</span> t: t.numpy().astype(np.uint8)),</span><br><span class="line">        transforms.ToPILImage(),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Take first image of batch</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(image.shape) == <span class="number">4</span>:</span><br><span class="line">        image = image[<span class="number">0</span>, :, :, :] </span><br><span class="line">    plt.imshow(reverse_transforms(image))</span><br><span class="line"></span><br><span class="line">data = load_transformed_dataset()</span><br><span class="line">dataloader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Simulate forward diffusion</span></span><br><span class="line">image = <span class="built_in">next</span>(<span class="built_in">iter</span>(dataloader))[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">15</span>))</span><br><span class="line">plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">num_images = <span class="number">10</span></span><br><span class="line">stepsize = <span class="built_in">int</span>(T/num_images)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, T, stepsize):</span><br><span class="line">    t = torch.Tensor([idx]).<span class="built_in">type</span>(torch.int64)</span><br><span class="line">    plt.subplot(<span class="number">1</span>, num_images+<span class="number">1</span>, math.ceil(idx/stepsize) + <span class="number">1</span>)</span><br><span class="line">    image, noise = forward_diffusion_sample(image, t)</span><br><span class="line">    show_tensor_image(image)</span><br></pre></td></tr></table></figure>
<h2><span id="step-2-the-backward-process-u-net">Step 2: The backward process = U-Net</span></h2><p>For a great introduction to UNets, have a look at this post: <a target="_blank" rel="noopener" href="https://amaarora.github.io/2020/09/13/unet.html">https://amaarora.github.io/2020/09/13/unet.html</a>.</p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>We use a simple form of a UNet for to predict the noise in the image</li>
<li>The input is a noisy image, the ouput the noise in the image</li>
<li>Because the parameters are shared accross time, we need to tell the network in which timestep we are</li>
<li>The Timestep is encoded by the transformer Sinusoidal Embedding</li>
<li>We output one single value (mean), because the variance is fixed</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_ch, out_ch, time_emb_dim, up=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)</span><br><span class="line">        <span class="keyword">if</span> up:</span><br><span class="line">            self.conv1 = nn.Conv2d(<span class="number">2</span>*in_ch, out_ch, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">            self.transform = nn.ConvTranspose2d(out_ch, out_ch, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv1 = nn.Conv2d(in_ch, out_ch, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">            self.transform = nn.Conv2d(out_ch, out_ch, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(out_ch, out_ch, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bnorm1 = nn.BatchNorm2d(out_ch)</span><br><span class="line">        self.bnorm2 = nn.BatchNorm2d(out_ch)</span><br><span class="line">        self.relu  = nn.ReLU()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, t, </span>):</span></span><br><span class="line">        <span class="comment"># First Conv</span></span><br><span class="line">        h = self.bnorm1(self.relu(self.conv1(x)))</span><br><span class="line">        <span class="comment"># Time embedding</span></span><br><span class="line">        time_emb = self.relu(self.time_mlp(t))</span><br><span class="line">        <span class="comment"># Extend last 2 dimensions</span></span><br><span class="line">        time_emb = time_emb[(..., ) + (<span class="literal">None</span>, ) * <span class="number">2</span>]</span><br><span class="line">        <span class="comment"># Add time channel</span></span><br><span class="line">        h = h + time_emb</span><br><span class="line">        <span class="comment"># Second Conv</span></span><br><span class="line">        h = self.bnorm2(self.relu(self.conv2(h)))</span><br><span class="line">        <span class="comment"># Down or Upsample</span></span><br><span class="line">        <span class="keyword">return</span> self.transform(h)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SinusoidalPositionEmbeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, time</span>):</span></span><br><span class="line">        device = time.device</span><br><span class="line">        half_dim = self.dim // <span class="number">2</span></span><br><span class="line">        embeddings = math.log(<span class="number">10000</span>) / (half_dim - <span class="number">1</span>)</span><br><span class="line">        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)</span><br><span class="line">        embeddings = time[:, <span class="literal">None</span>] * embeddings[<span class="literal">None</span>, :]</span><br><span class="line">        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Double check the ordering here</span></span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleUnet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A simplified variant of the Unet architecture.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        image_channels = <span class="number">3</span></span><br><span class="line">        down_channels = (<span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>, <span class="number">1024</span>)</span><br><span class="line">        up_channels = (<span class="number">1024</span>, <span class="number">512</span>, <span class="number">256</span>, <span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        out_dim = <span class="number">1</span> </span><br><span class="line">        time_emb_dim = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Time embedding</span></span><br><span class="line">        self.time_mlp = nn.Sequential(</span><br><span class="line">                SinusoidalPositionEmbeddings(time_emb_dim),</span><br><span class="line">                nn.Linear(time_emb_dim, time_emb_dim),</span><br><span class="line">                nn.ReLU()</span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initial projection</span></span><br><span class="line">        self.conv0 = nn.Conv2d(image_channels, down_channels[<span class="number">0</span>], <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Downsample</span></span><br><span class="line">        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+<span class="number">1</span>], \</span><br><span class="line">                                    time_emb_dim) \</span><br><span class="line">                    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(down_channels)-<span class="number">1</span>)])</span><br><span class="line">        <span class="comment"># Upsample</span></span><br><span class="line">        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+<span class="number">1</span>], \</span><br><span class="line">                                        time_emb_dim, up=<span class="literal">True</span>) \</span><br><span class="line">                    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(up_channels)-<span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line">        self.output = nn.Conv2d(up_channels[-<span class="number">1</span>], <span class="number">3</span>, out_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, timestep</span>):</span></span><br><span class="line">        <span class="comment"># Embedd time</span></span><br><span class="line">        t = self.time_mlp(timestep)</span><br><span class="line">        <span class="comment"># Initial conv</span></span><br><span class="line">        x = self.conv0(x)</span><br><span class="line">        <span class="comment"># Unet</span></span><br><span class="line">        residual_inputs = []</span><br><span class="line">        <span class="keyword">for</span> down <span class="keyword">in</span> self.downs:</span><br><span class="line">            x = down(x, t)</span><br><span class="line">            residual_inputs.append(x)</span><br><span class="line">        <span class="keyword">for</span> up <span class="keyword">in</span> self.ups:</span><br><span class="line">            residual_x = residual_inputs.pop()</span><br><span class="line">            <span class="comment"># Add residual x as additional channels</span></span><br><span class="line">            x = torch.cat((x, residual_x), dim=<span class="number">1</span>)           </span><br><span class="line">            x = up(x, t)</span><br><span class="line">        <span class="keyword">return</span> self.output(x)</span><br><span class="line"></span><br><span class="line">model = SimpleUnet()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Num params: &quot;</span>, <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters()))</span><br><span class="line">model</span><br></pre></td></tr></table></figure>
<p><strong>Further improvements that can be implemented:</strong></p>
<ul>
<li>Residual connections</li>
<li>Different activation functions like SiLU, GWLU, â€¦</li>
<li>BatchNormalization </li>
<li>GroupNormalization</li>
<li>Attention</li>
<li>â€¦</li>
</ul>
<h2><span id="step-3-the-loss">Step 3: The loss</span></h2><p><strong>Key Takeaways:</strong></p>
<ul>
<li>After some maths we end up with a very simple loss function</li>
<li>There are other possible choices like L2 loss ect.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_loss</span>(<span class="params">model, x_0, t</span>):</span></span><br><span class="line">    x_noisy, noise = forward_diffusion_sample(x_0, t, device)</span><br><span class="line">    noise_pred = model(x_noisy, t)</span><br><span class="line">    <span class="keyword">return</span> F.l1_loss(noise, noise_pred)</span><br></pre></td></tr></table></figure>
<h2><span id="sampling">Sampling</span></h2><ul>
<li>Without adding @torch.no_grad() we quickly run out of memory, because pytorch tacks all the previous images for gradient calculation </li>
<li>Because we pre-calculated the noise variances for the forward pass, we also have to use them when we sequentially perform the backward process</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_timestep</span>(<span class="params">x, t</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Calls the model to predict the noise in the image and returns </span></span><br><span class="line"><span class="string">    the denoised image. </span></span><br><span class="line"><span class="string">    Applies noise to this image, if we are not in the last step yet.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    betas_t = get_index_from_list(betas, t, x.shape)</span><br><span class="line">    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(</span><br><span class="line">        sqrt_one_minus_alphas_cumprod, t, x.shape</span><br><span class="line">    )</span><br><span class="line">    sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Call model (current image - noise prediction)</span></span><br><span class="line">    model_mean = sqrt_recip_alphas_t * (</span><br><span class="line">        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t</span><br><span class="line">    )</span><br><span class="line">    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> t == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> model_mean</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        noise = torch.randn_like(x)</span><br><span class="line">        <span class="keyword">return</span> model_mean + torch.sqrt(posterior_variance_t) * noise </span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_plot_image</span>():</span></span><br><span class="line">    <span class="comment"># Sample noise</span></span><br><span class="line">    img_size = IMG_SIZE</span><br><span class="line">    img = torch.randn((<span class="number">1</span>, <span class="number">3</span>, img_size, img_size), device=device)</span><br><span class="line">    plt.figure(figsize=(<span class="number">15</span>,<span class="number">15</span>))</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    num_images = <span class="number">10</span></span><br><span class="line">    stepsize = <span class="built_in">int</span>(T/num_images)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,T)[::-<span class="number">1</span>]:</span><br><span class="line">        t = torch.full((<span class="number">1</span>,), i, device=device, dtype=torch.long)</span><br><span class="line">        img = sample_timestep(img, t)</span><br><span class="line">        <span class="keyword">if</span> i % stepsize == <span class="number">0</span>:</span><br><span class="line">            plt.subplot(<span class="number">1</span>, num_images, <span class="built_in">int</span>(i/stepsize)+<span class="number">1</span>)</span><br><span class="line">            show_tensor_image(img.detach().cpu())</span><br><span class="line">    plt.show()            </span><br></pre></td></tr></table></figure>
<h2><span id="training">Training</span></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"></span><br><span class="line">model.to(device)</span><br><span class="line">optimizer = Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">epochs = <span class="number">100</span> <span class="comment"># Try more!</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> step, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">      optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">      t = torch.randint(<span class="number">0</span>, T, (BATCH_SIZE,), device=device).long()</span><br><span class="line">      loss = get_loss(model, batch[<span class="number">0</span>], t)</span><br><span class="line">      loss.backward()</span><br><span class="line">      optimizer.step()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> epoch % <span class="number">5</span> == <span class="number">0</span> <span class="keyword">and</span> step == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span> | step <span class="subst">&#123;step:03d&#125;</span> Loss: <span class="subst">&#123;loss.item()&#125;</span> &quot;</span>)</span><br><span class="line">        sample_plot_image()</span><br></pre></td></tr></table></figure>
<p>In Table 2, we show the sample quality effects of reverse process parameterizations and training<br>objectives (Section 3.2). We find that the baseline option of predicting ÂµËœ works well only when<br>trained on the true variational bound instead of unweighted mean squared error, a simplified objective<br>akin to Eq. (14). We also see that learning reverse process variances (by incorporating a parameterized<br>diagonal Î£Î¸(xt) into the variational bound) leads to unstable training and poorer sample quality<br>compared to fixed variances. Predicting , as we proposed, performs approximately as well as<br>predicting ÂµËœ when trained on the variational bound with fixed variances, but much better when trained<br>with our simplified objective.</p>
<p>iffusion models scale down the data with each forward process step (by a âˆš<br>1 âˆ’ Î²t factor)<br>so that variance does not grow when adding noise, thus providing consistently scaled inputs<br>to the neural net reverse process. NCSN omits this scaling factor.</p>
<h1><span id="ways-to-improve">Ways to Improve</span></h1><ul>
<li>loss function</li>
<li>value of $\beta_t$</li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>Diffusion model</p><p><a href="http://example.com/2023/04/03/Diffusion-model/">http://example.com/2023/04/03/Diffusion-model/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Yuzi Liang</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2023-04-03</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2023-04-08</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Machine-Learning/">Machine Learning</a><a class="link-muted mr-2" rel="tag" href="/tags/PyTorch/">PyTorch</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2023/04/11/CNN-with-Pytorch/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">CNN with Pytorch</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2022/11/07/TensorFlow-Tutorial/"><span class="level-item">TensorFlow Tutorial</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://example.com/2023/04/03/Diffusion-model/';
            this.page.identifier = '2023/04/03/Diffusion-model/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'lyz' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-09-28T00:13:37.000Z">2023-09-27</time></p><p class="title"><a href="/2023/09/27/Image-Generator-Application-and-Implementation-of-StyleGAN-1/">Image Generator: Application and Implementation of StyleGAN</a></p><p class="categories"><a href="/categories/Project-Summary/">Project Summary</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-04-11T22:35:16.000Z">2023-04-11</time></p><p class="title"><a href="/2023/04/11/CNN-with-Pytorch/">CNN with Pytorch</a></p><p class="categories"><a href="/categories/Learning-Notes/">Learning Notes</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-04-04T04:22:22.000Z">2023-04-03</time></p><p class="title"><a href="/2023/04/03/Diffusion-model/">Diffusion model</a></p><p class="categories"><a href="/categories/Learning-Notes/">Learning Notes</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-11-08T02:10:30.000Z">2022-11-07</time></p><p class="title"><a href="/2022/11/07/TensorFlow-Tutorial/">TensorFlow Tutorial</a></p><p class="categories"><a href="/categories/Learning-Notes/">Learning Notes</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-05-28T22:58:52.000Z">2022-05-28</time></p><p class="title"><a href="/2022/05/28/Lattice-Boltzmann-Method/">Lattice Boltzmann Method</a></p></div></article></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Demo/"><span class="level-start"><span class="level-item">Demo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Learning-Notes/"><span class="level-start"><span class="level-item">Learning Notes</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Project-Summary/"><span class="level-start"><span class="level-item">Project Summary</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">14</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Magnetics/"><span class="tag">Magnetics</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Monte-Carlo-Method/"><span class="tag">Monte Carlo Method</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PyTorch/"><span class="tag">PyTorch</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Selenium/"><span class="tag">Selenium</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TensorFlow/"><span class="tag">TensorFlow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Thermoelectricity/"><span class="tag">Thermoelectricity</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Web-Crawler/"><span class="tag">Web Crawler</span><span class="tag">3</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/ustc_logo.jpg" alt="Yuzi Liang | University of Science and Technology of China" height="28"></a><p class="is-size-7"><span>&copy; 2023 Yuzi Liang</span>Â Â Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>Â &amp;Â <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">Ã—</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>