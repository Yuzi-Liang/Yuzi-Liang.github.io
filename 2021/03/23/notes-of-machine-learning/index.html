<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Notes of Machine Learning(P1-P11) - Yuzi Liang | University of Science and Technology of China</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Yuzi Liang | University of Science and Technology of China"><meta name="msapplication-TileImage" content="/img/favicon2.ico"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Yuzi Liang | University of Science and Technology of China"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Machine LearningYuzi Liang   1.Regression Step 1:Model Step 2: Goodness of function Step 3: Best Function Step 4: Gradient Descent Adaptive Learning Rates Adagrad Stochastic Gradient Descent Feature S"><meta property="og:type" content="blog"><meta property="og:title" content="Notes of Machine Learning(P1-P11)"><meta property="og:url" content="http://example.com/2021/03/23/notes-of-machine-learning/"><meta property="og:site_name" content="Yuzi Liang | University of Science and Technology of China"><meta property="og:description" content="Machine LearningYuzi Liang   1.Regression Step 1:Model Step 2: Goodness of function Step 3: Best Function Step 4: Gradient Descent Adaptive Learning Rates Adagrad Stochastic Gradient Descent Feature S"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/images/cf.png"><meta property="og:image" content="http://example.com/images/sigma.png"><meta property="og:image" content="http://example.com/images/fc.png"><meta property="og:image" content="http://example.com/images/ddd.png"><meta property="og:image" content="http://example.com/images/ppp.png"><meta property="og:image" content="http://example.com/images/tl.png"><meta property="og:image" content="http://example.com/images/tl2.png"><meta property="og:image" content="http://example.com/images/tl3.png"><meta property="article:published_time" content="2021-03-23T20:32:39.000Z"><meta property="article:modified_time" content="2023-09-25T21:33:56.768Z"><meta property="article:author" content="Yuzi Liang"><meta property="article:tag" content="Machine Learning"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/images/cf.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2021/03/23/notes-of-machine-learning/"},"headline":"Yuzi Liang | University of Science and Technology of China","image":["http://example.com/images/cf.png","http://example.com/images/sigma.png","http://example.com/images/fc.png","http://example.com/images/ddd.png","http://example.com/images/ppp.png","http://example.com/images/tl.png","http://example.com/images/tl2.png","http://example.com/images/tl3.png"],"datePublished":"2021-03-23T20:32:39.000Z","dateModified":"2023-09-25T21:33:56.768Z","author":{"@type":"Person","name":"Yuzi Liang"},"description":"Machine LearningYuzi Liang   1.Regression Step 1:Model Step 2: Goodness of function Step 3: Best Function Step 4: Gradient Descent Adaptive Learning Rates Adagrad Stochastic Gradient Descent Feature S"}</script><link rel="canonical" href="http://example.com/2021/03/23/notes-of-machine-learning/"><link rel="icon" href="/img/favicon2.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/ustc_logo.jpg" alt="Yuzi Liang | University of Science and Technology of China" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-03-23T20:32:39.000Z" title="3/23/2021, 1:32:39 PM">2021-03-23</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-09-25T21:33:56.768Z" title="9/25/2023, 2:33:56 PM">2023-09-25</time></span><span class="level-item"><a class="link-muted" href="/categories/Learning-Notes/">Learning Notes</a></span><span class="level-item">9 minutes read (About 1394 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Notes of Machine Learning(P1-P11)</h1><div class="content"><h1><span id="machine-learning">Machine Learning</span></h1><p>Yuzi Liang</p>
<!-- toc -->
<ul>
<li><a href="#1regression">1.Regression</a><ul>
<li><a href="#step-1model">Step 1:Model</a></li>
<li><a href="#step-2-goodness-of-function">Step 2: Goodness of function</a></li>
<li><a href="#step-3-best-function">Step 3: Best Function</a></li>
<li><a href="#step-4-gradient-descent">Step 4: Gradient Descent</a><ul>
<li><a href="#adaptive-learning-rates"><em>Adaptive Learning Rates</em></a></li>
<li><a href="#adagrad">Adagrad</a></li>
<li><a href="#stochastic-gradient-descent"><em>Stochastic Gradient Descent</em></a></li>
<li><a href="#feature-scaling"><em>Feature Scaling</em></a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#2optimization-for-deep-learning">2.Optimization for Deep Learning</a><ul>
<li><a href="#sgd-with-momentumsgdm">SGD with Momentum(SGDM)</a></li>
<li><a href="#rmsprop">RMSProp</a></li>
<li><a href="#adam">Adam</a></li>
<li><a href="#optimizer-real-application"><em>Optimizer: Real Application</em></a></li>
<li><a href="#adam-vs-sgdm"><em>Adam vs SGDM</em></a></li>
</ul>
</li>
<li><a href="#3classification-probabilistic-generative-model">3.Classification: Probabilistic Generative Model</a><ul>
<li><a href="#how-to-do-classfication">How to do Classfication</a></li>
<li><a href="#ideal-alternatives">Ideal Alternatives</a></li>
<li><a href="#generative-model">Generative Model</a><ul>
<li><a href="#two-classes">Two Classes</a></li>
</ul>
</li>
<li><a href="#three-steps">Three Steps</a><ul>
<li><a href="#function-setmodel">Function Set(Model)</a></li>
<li><a href="#goodness-of-a-function">Goodness of a function</a></li>
<li><a href="#find-the-best-function-easy">Find the best function: easy</a></li>
</ul>
</li>
<li><a href="#logistic-regression">Logistic Regression</a><ul>
<li><a href="#function-set">Function Set</a></li>
<li><a href="#goodness-of-a-function">Goodness of a Function</a></li>
<li><a href="#find-the-best-function">Find the Best Function</a></li>
<li><a href="#discriminative-vs-generative">Discriminative v.s. Generative</a></li>
<li><a href="#multi-class-classification">Multi-class Classification</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- tocstop -->
<h2><span id="1regression">1.Regression</span></h2><h3><span id="step-1model">Step 1:Model</span></h3><p>Choose a model to generate results.</p>
<p>E.g. Linear model</p>
<script type="math/tex; mode=display">
y = b +\sum w_i x_i</script><p>$x_i$: feature, $w_i$: weight, $b$: bias</p>
<h3><span id="step-2-goodness-of-function">Step 2: Goodness of function</span></h3><p>value by the Loss Function.</p>
<script type="math/tex; mode=display">
L(f) = \sum^N_{n=1} (\hat y^n - f(x^n))^2</script><p>Loss Function is a function of function.</p>
<ul>
<li><em>Regularization</em></li>
</ul>
<script type="math/tex; mode=display">
y = b + \sum w_i x_i</script><script type="math/tex; mode=display">
L = \sum_n(\hat y - (b + \sum w_i x_i))^2 + \lambda \sum(w_i)^2</script><p>Larger $\lambda$ means smoother curve.</p>
<h3><span id="step-3-best-function">Step 3: Best Function</span></h3><p>Pick the best function.</p>
<script type="math/tex; mode=display">
f^* = arg \ \textrm{min} \ L(f)</script><p>$ arg $ $\textrm{min}$: argument of the minimum, that is to say, the set of points(functions) of the given for which the value of the given expression attains its minimum value.</p>
<h3><span id="step-4-gradient-descent">Step 4: Gradient Descent</span></h3><p>How to find the Best Function.</p>
<p>Still use the example of linear model, where the $L$ is the function of $w$:</p>
<script type="math/tex; mode=display">
w^* = arg \ \min_w \ L(w)</script><p>choose a value $w_0$:</p>
<script type="math/tex; mode=display">
w^1 \leftarrow w^0 - \eta \frac{dL}{dw}|_{w = w^0}</script><script type="math/tex; mode=display">
w^2 \leftarrow w^1 - \eta \frac{dL}{dw}|_{w = w^1}</script><p>$\eta$ is called “learning rate”</p>
<p>Q: How about two parameters?</p>
<p>A: Use gradient.</p>
<script type="math/tex; mode=display">
\vec{w}^{i+1} = \vec{w}^i - \eta \nabla L</script><p>Some tips:</p>
<h4><span id="adaptive-learning-rates"><em>Adaptive Learning Rates</em></span></h4><p>Popular &amp; Simple Idea: Reduce the learning rate by some factor every few epochs</p>
<p>E.g. $1/t \ decay: \eta^t = \eta/\sqrt{t+1} $</p>
<font color="red">Learning rate cannot be one-size-fits-all</font>

<h4><span id="adagrad">Adagrad</span></h4><p>Divide the learning rate of each parameter by the root mean square of its previous derivatives.</p>
<script type="math/tex; mode=display">
w^{t+1} \leftarrow w^t -\frac{\eta^t}{\sigma^t}g^t</script><script type="math/tex; mode=display">
\sigma^t = \sqrt{\frac{1}{t+1}\sum^t_{i=0}(g^i)^2}</script><p>$g^t = \frac{\partial L(\theta^t)}{\partial w}$, noticed that $\eta^t,\sigma^t$ both have factor $1/\sqrt{t+1}$ </p>
<script type="math/tex; mode=display">
w^{t+1} \leftarrow w^t - \frac{\eta}{\sqrt{\sum^t_{i=0}(g^i)^2}}g^t</script><h4><span id="stochastic-gradient-descent"><em>Stochastic Gradient Descent</em></span></h4><p>When use gradient descent:</p>
<script type="math/tex; mode=display">
L = \sum_n(\hat{y}^n-(b+\sum_iw_ix^n_i))^2</script><script type="math/tex; mode=display">
\theta^i = \theta^{i-1} - \eta \nabla L(\theta^{i-1})</script><p>Stochastic Gradient Descent is to say:</p>
<p>Pick an example $x^n$(random or sequential)</p>
<script type="math/tex; mode=display">
L = (\hat{y}^n-(b+\sum_iw_ix^n_i))^2</script><p>faster than gradient descent when there are many examples.</p>
<h4><span id="feature-scaling"><em>Feature Scaling</em></span></h4><p>E.g. </p>
<script type="math/tex; mode=display">
y = b + w_1x_1 + w_2x_2</script><p>sometimes $x_1 \ll x_2$, then $L$ will be smooth in $w_1$ direction, sharp in $w_2$ direction.</p>
<p>We need to rescale $x_1,x_2$. Usually we use:</p>
<script type="math/tex; mode=display">
x_i^r \leftarrow \frac{x_i^r-m_i}{\sigma_i}</script><p>$m_i$: mean, $\sigma_i$: standard deviation</p>
<h2><span id="2optimization-for-deep-learning">2.Optimization for Deep Learning</span></h2><h3><span id="sgd-with-momentumsgdm">SGD with Momentum(SGDM)</span></h3><p>Start at: point $\theta^0$, movement $v^0 = 0$, use $ v $ to represent the momentum.</p>
<p>Next step: $v^1 = \lambda v^0 - \eta \nabla L(\theta^0)$, move to $\theta^1 = \theta^0 + v^1$</p>
<p>Next step: $v^2 = \lambda v^1 - \eta \nabla L(\theta^1)$, move to $\theta^2 = \theta^1 + v^2$</p>
<p>……</p>
<font color="red">Movement not just based on gradient, but previous movement.</font>

<h3><span id="rmsprop">RMSProp</span></h3><script type="math/tex; mode=display">
\theta_t = \theta_{t-1}-\frac{\eta}{\sqrt{v_t}}(g_{t-1})^2</script><script type="math/tex; mode=display">
v_1 = g^2_0</script><script type="math/tex; mode=display">
v_t = \alpha v_{t-1} + (1-\alpha)(g_{t-1})^2</script><p>so that $v_t$ will not increase constantly.</p>
<h3><span id="adam">Adam</span></h3><p>We have</p>
<p>SGDM:</p>
<script type="math/tex; mode=display">
\theta_t = \theta_{t-1} - \eta m_t</script><script type="math/tex; mode=display">
m_t = \beta_1 m_{t-1} + (1-\beta_1)g_{t-1}</script><p>RMSProp:</p>
<script type="math/tex; mode=display">
\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{v_t}}g_{t-1}</script><script type="math/tex; mode=display">
v_1 = g_0^2</script><script type="math/tex; mode=display">
v_t = \beta_2 v_{t-1} + (1-\beta_2)(g_{t-1})^2</script><p>Combine SGDM with RMSProp, we get Adam:</p>
<script type="math/tex; mode=display">
\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{\hat{v_t}}+\epsilon}\hat{m_t}</script><script type="math/tex; mode=display">
\hat{m_t} = \frac{m_t}{1-\beta^t_1}</script><script type="math/tex; mode=display">
v_t = \frac{v_t}{1-\beta_2^t}</script><p>The collective name of Adagrad, RMSProp and Adam is Adaptive learning rate.</p>
<h3><span id="optimizer-real-application"><em>Optimizer: Real Application</em></span></h3><p>E.g. BERT, Transformer, Tacotron(Speech Synthesis), YOLO(CV), Mask R-CNN, ResNet, Big-GAN, MAML(Classfication)</p>
<h3><span id="adam-vs-sgdm"><em>Adam vs SGDM</em></span></h3><p>Adam: fast training, large generalization gap, unstable.</p>
<p>SGDM: stable, little generation gap, better convergence(?)</p>
<p>Can we simply combine Adam with SGDM?</p>
<p>We have SWATS: Begin with Adam(fast), end with SGDM. But we need to consider the change criteria and we can not get a strict criteria.</p>
<p>(The TA is making me very sleepy, I have to skip this class for the moment)</p>
<h2><span id="3classification-probabilistic-generative-model">3.Classification: Probabilistic Generative Model</span></h2><h3><span id="how-to-do-classfication">How to do Classfication</span></h3><p>If we simply use linear model(Regression):</p>
<center class="full">
<img src="/images/cf.png" width="80%">
</center>


<p>We may get the purple line instead of the green line because of the fitting need the result reach the minimal.</p>
<h3><span id="ideal-alternatives">Ideal Alternatives</span></h3><p>Function(Model):</p>
<script type="math/tex; mode=display">
x \rightarrow \left\{
\begin{aligned}
& g(x)>0 &  & \rm{Output} = \rm{Class1} \\
& else &  & \rm{Output} = \rm{Class2}
\end{aligned}
\right.</script><p>Loss function:</p>
<script type="math/tex; mode=display">
L(f) = \sum_n \delta(f(x^n) \neq \hat{y}^n)</script><p>means the number of times $f$ get incorrect results on training data.</p>
<p>Since L is not  differentiable, we can not use Gradient Descent. It seems like that we can use Perception or SVM, but we will not talk about it today.</p>
<h3><span id="generative-model">Generative Model</span></h3><h4><span id="two-classes">Two Classes</span></h4><p>Class1:$P(C_1) \ , \ P(x|C_1)$</p>
<p>Class2:$P(C_2) \ , \ P(x|C_2)$</p>
<p>We have:</p>
<script type="math/tex; mode=display">
P(C_1|x) = \frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}</script><p>Generative Model: $P(x) = P(x|C_1)P(C_1)+P(x|C_2)P(C_2)$</p>
<p> $x$的分布取最大似然估计.</p>
<p>若取$x$分布为高斯分布,大部分情况下不会分别讨论每一个类的方差，而是考虑一个统一的covariance matrix.</p>
<p>综合方差的计算 $\Sigma = $权重 $\times \Sigma^i$的求和.权重就是数据量之比.</p>
<h3><span id="three-steps">Three Steps</span></h3><h4><span id="function-setmodel">Function Set(Model)</span></h4><p>parameters of model: $P(C_1) \ , \ P(x|C_1) \ , \ P(C_2) \ , \ P(x|C_2)$</p>
<script type="math/tex; mode=display">
P(C_1|x) = \frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}</script><p>if $P(C_1|x)&gt;0.5$, output: class 1</p>
<p>Otherwise, output: class 2</p>
<script type="math/tex; mode=display">
P(C_1|x) = \frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}
\\ = \frac{1}{1+\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}} = \frac{1}{1+exp(-z)} = \sigma(z)</script><p>其中:</p>
<script type="math/tex; mode=display">
z = ln\frac{P(x|C_1)P(C_1)}{P(x|C_2)P(C_2)}</script><p>$\sigma(z)$的图像如下:</p>
<center class="full">
<img src="/images/sigma.png" width="80%">
</center>




<p>若取统一的方差,经过化简可以得到:</p>
<script type="math/tex; mode=display">
z = w \cdot x + b = \sum_i w_i x_i + b</script><p>分界线就变成了一条直线(或者是$n-1$维超平面)</p>
<h4><span id="goodness-of-a-function">Goodness of a function</span></h4><h4><span id="find-the-best-function-easy">Find the best function: easy</span></h4><h3><span id="logistic-regression">Logistic Regression</span></h3><h4><span id="function-set">Function Set</span></h4><script type="math/tex; mode=display">
f_{w,b}(x) = \sigma(\sum_i w_i x_i + b)</script><h4><span id="goodness-of-a-function">Goodness of a Function</span></h4><p>Cross entropy:</p>
<script type="math/tex; mode=display">
C(f(x^n),\hat{y}^n) = -[\hat{y}^nlnf(x^n)+(1-\hat{y}^n)ln(1-f(x^n))]</script><p>其中:</p>
<script type="math/tex; mode=display">
\hat y^n: 1 \ \rm{for \ class} \ 1, \ 0 \ \rm{for \ class} \ 2</script><p>需要使交叉熵之和最小,即</p>
<script type="math/tex; mode=display">
L(f) = \sum_n C(f(x^n),\hat y^n)</script><p>最小.</p>
<h4><span id="find-the-best-function">Find the Best Function</span></h4><p>求微分,用 Gradient Descent</p>
<script type="math/tex; mode=display">
\frac{\partial L(w,b)}{\partial w_i} = \sum_n -(\hat y^n - f_{w,b}(x^n))x_i^n</script><p>后面讨论了选取交叉熵而不是像之前一样选择方差的原因:若选择方差,在远离最优解的地方导数也很小,导致不好选取步长</p>
<center class="full">
<img src="/images/fc.png" width="80%">
</center>


<h4><span id="discriminative-vs-generative">Discriminative v.s. Generative</span></h4><center class="full">
<img src="/images/ddd.png" width="80%">
</center>


<p>这两种方法寻找到的结果是不一样的.一般情况下前者效果更好,Generative有额外假设,受data影响较小.</p>
<h4><span id="multi-class-classification">Multi-class Classification</span></h4><p>3 classes as example</p>
<script type="math/tex; mode=display">
C_1:w^1,b_1 \ \ \ z_1 = w^1 \cdot x + b_1\\
C_2:w^2,b_2 \ \ \ z_2 = w^2 \cdot x + b_2\\
C_3:w^3,b_3 \ \ \ z_3 = w^3 \cdot x + b_3</script><center class="full">
<img src="/images/ppp.png" width="80%">
</center>

<center class="full">
<img src="/images/tl.png" width="80%">
</center>

<p><br>随后使交叉熵每个分量最小即可.<br><br><br><br></p>
<h4><span id="limitation-of-logistic-regression">Limitation of Logistic Regression</span></h4>

<p>有些分类没法做,但是可以利用Feature Transformation(有些Heuristic Ad Hoc 启发性和临时性的东西)解决.</p>
<center class="full">
<img src="/images/tl2.png" width="80%">
</center>

<p>从而衍生出来了级联逻辑回归模型.</p>
<center class="full">
<img src="/images/tl3.png" width="80%">
</center>

<p>进入Deep Learning.</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Notes of Machine Learning(P1-P11)</p><p><a href="http://example.com/2021/03/23/notes-of-machine-learning/">http://example.com/2021/03/23/notes-of-machine-learning/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Yuzi Liang</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2021-03-23</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2023-09-25</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Machine-Learning/">Machine Learning</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2021/03/23/stock-price-forecast-based-on-Monte-Carlo-method-and-machine-learning/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Stock Price Forecast Based on Monte Carlo Method and Machine Learning</span></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://example.com/2021/03/23/notes-of-machine-learning/';
            this.page.identifier = '2021/03/23/notes-of-machine-learning/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'lyz' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-10-05T00:37:56.000Z">2023-10-04</time></p><p class="title"><a href="/2023/10/04/Prediction-of-house-sale-prices-for-King-County/">Prediction of house sale prices for King County</a></p><p class="categories"><a href="/categories/Project-Summary/">Project Summary</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-09-28T00:13:37.000Z">2023-09-27</time></p><p class="title"><a href="/2023/09/27/Image-Generator-Application-and-Implementation-of-StyleGAN-1/">Image Generator: Application and Implementation of StyleGAN</a></p><p class="categories"><a href="/categories/Project-Summary/">Project Summary</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-04-11T22:35:16.000Z">2023-04-11</time></p><p class="title"><a href="/2023/04/11/CNN-with-Pytorch/">CNN with Pytorch</a></p><p class="categories"><a href="/categories/Learning-Notes/">Learning Notes</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-04-04T04:22:22.000Z">2023-04-03</time></p><p class="title"><a href="/2023/04/03/Diffusion-model/">Diffusion model</a></p><p class="categories"><a href="/categories/Learning-Notes/">Learning Notes</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-11-08T02:10:30.000Z">2022-11-07</time></p><p class="title"><a href="/2022/11/07/TensorFlow-Tutorial/">TensorFlow Tutorial</a></p><p class="categories"><a href="/categories/Learning-Notes/">Learning Notes</a></p></div></article></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Demo/"><span class="level-start"><span class="level-item">Demo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Learning-Notes/"><span class="level-start"><span class="level-item">Learning Notes</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Project-Summary/"><span class="level-start"><span class="level-item">Project Summary</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">15</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Magnetics/"><span class="tag">Magnetics</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Monte-Carlo-Method/"><span class="tag">Monte Carlo Method</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PyTorch/"><span class="tag">PyTorch</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Selenium/"><span class="tag">Selenium</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TensorFlow/"><span class="tag">TensorFlow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Thermoelectricity/"><span class="tag">Thermoelectricity</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Web-Crawler/"><span class="tag">Web Crawler</span><span class="tag">3</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/ustc_logo.jpg" alt="Yuzi Liang | University of Science and Technology of China" height="28"></a><p class="is-size-7"><span>&copy; 2023 Yuzi Liang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>